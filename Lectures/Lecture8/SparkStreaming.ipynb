{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460dfb6e-7df2-451a-8fc0-302f36167879",
   "metadata": {},
   "source": [
    "# `Промышленное машинное обучение на Spark`\n",
    "## `Занятие 08: Spark Structured Streaming`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23badb3-e3e8-454a-aa85-fcc81baa048d",
   "metadata": {},
   "source": [
    "![sparkStreaming](images/sparkStreaming.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadc31e-7e74-488b-96bf-e69213201797",
   "metadata": {},
   "source": [
    "О чём можно узнать из этого ноутбука:\n",
    "\n",
    "* Обработка данных в онлайн-режиме\n",
    "* Подключение источников\n",
    "* Spark Streaming: Filters, Join, Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13b6be-28ca-4187-981e-81030844fc4d",
   "metadata": {},
   "source": [
    "Произведём все необходимые импорты и создадим Spark-context для дальнейшей работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e17c15-e216-473d-85b0-854bd86177b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 07:40:11 WARN Utils: Your hostname, vm-01 resolves to a loopback address: 127.0.1.1; using 10.128.0.16 instead (on interface eth0)\n",
      "23/12/02 07:40:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/02 07:40:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "        .set('spark.ui.port', '4050')\n",
    "        .set('spark.driver.memory', '6g')\n",
    "        .set('spark.executor.extraJavaOptions', '-Xss512m')\n",
    "        .set('spark.driver.extraJavaOptions', '-Xss512m')\n",
    "        .setMaster('local[*]')\n",
    "        .setAppName(\"StructuredNetworkWordCount\")\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186deac-698e-4179-b512-804a8f9e5b5b",
   "metadata": {},
   "source": [
    "Далее создадим специальный датафрейм, в который будут приходить данные по сети. Этот датафрейм представляет собой абстракцию таблицы с бесконечным числом строк, в котором все пришедшие данные располагаются в колонке `value`. \n",
    "\n",
    "Вызов метода `readStream` указывает на то, что созданный датафрейм будет потокового типа, также при создании необходимо указать откуда будут идти данные. Источниками данных могут быть:\n",
    "* Открытыте сетевые соединения\n",
    "* Дириктории в локальной файловой системе или hdfs\n",
    "* Из брокеров сообщений, например Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c892c0fb-d403-402e-bf84-8ef14852e4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 07:48:58 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "words_lines = (spark\n",
    "        .readStream\n",
    "        .format('socket') # аргумент socket указывает на то, что данные будут приходить по сети\n",
    "        .option('host', 'localhost') # указываем на каком хосте располагаются данные\n",
    "        .option('port', '10000') # указываем на каком порте нужно вычитывать их\n",
    "        .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9930fc-13c1-4382-a93e-cd1407774478",
   "metadata": {},
   "source": [
    "Сделаем вывод схемы данных и типа созданного датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "237de728-abc5-404c-ae60-723fdc868cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Streaming DataFame: True'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lines.printSchema()\n",
    "\n",
    "f\"Streaming DataFame: {words_lines.isStreaming}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f5c11-7220-4c9e-984c-117a408092e4",
   "metadata": {},
   "source": [
    "Далее уже можно приступать непосредственно к описании требуемой логики обработки данных. В ячейке ниже происходит операция подсчёта пришедших слов, для этого разбиваем строки на списки слов и представляем каждое слово в виде отдельной строки в абстрактной таблице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f460fb27-b989-4619-8b8f-12c7981086b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words_lines.select(\n",
    "        F.explode( # преобразуем пришедшие данные в список и каждый элемент списка преобразуем в строку датафрейма\n",
    "            F.split(words_lines.value, ' ') # атрибут .value - данные которые приходят на host:port\n",
    "        ).alias('word')\n",
    "    )\n",
    "\n",
    "wordCounts = words.groupBy('word').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fadf79a-b522-4471-961b-e1d78d67f7f4",
   "metadata": {},
   "source": [
    "В предыдущих ячейках были сформировны два из трёх шагов: вычитка и процессеинг. Теперь сформируем последний шаг - запись. В нём указываем куда пишем данные - параметр метода format, и по какой логике формируется вывод - метод outputMode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a120132-88d2-4d81-83fc-1e9850bf5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (wordCounts\n",
    "        .writeStream\n",
    "        .outputMode('complete')\n",
    "        .format('console')\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa22b87-31bf-4a97-995d-9ab3f6e323e0",
   "metadata": {},
   "source": [
    "Подобно тому как исполнение операций в batch-режиме производлось после вызова action операций, в SparkStreaming вычитка новых данных происходит после запуска метода awaitTermination, в котором производится старт фоновой задачи на обновление."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bee1d9-dd25-48f8-92c1-a40cca6012bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ac69a-35ed-4cbc-a29a-2c9c8c79cf45",
   "metadata": {},
   "source": [
    "## Чтение данных из файловой системы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b00539-567e-4ead-ab1a-8a2c3113f7be",
   "metadata": {},
   "source": [
    "Попробуем считать данные из файловой системы, для этого рекомендуется заранее описать схему данных, а затем указать, какую директорию в файловой системе необходимо мониторить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa34ffe-cf39-4e34-9a02-be569f4e3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# описываем схему данных таблиц из директории data\n",
    "userSchema = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", StringType(), True),\n",
    "        StructField(\"Open\", DoubleType(), True),\n",
    "        StructField(\"High\", DoubleType(), True),\n",
    "        StructField(\"Low\", DoubleType(), True),\n",
    "        StructField(\"Close\", DoubleType(), True),\n",
    "        StructField(\"Adjusted Close\", DoubleType(), True),\n",
    "        StructField(\"Volume\", DoubleType(), True),\n",
    "    ]\n",
    ")\n",
    "# Так как информация о названии компании содержится\n",
    "# в названии файла, то добавим дополнительное поле в \n",
    "# датафрейм из названия файла\n",
    "def get_company():\n",
    "    filename = F.element_at(\n",
    "        F.split(F.input_file_name(), \"/\"), -1\n",
    "    )\n",
    "    return F.element_at(F.split(filename, \"_\"), 1)\n",
    "\n",
    "initDF = (spark\n",
    "  .readStream\n",
    "  .format(\"csv\")\n",
    "  .option(\"maxFilesPerTrigger\", 2) # максимальное кол-во одновременно считываемых файлов\n",
    "  .option(\"header\", True)\n",
    "  .option(\"path\", \"sourceDir\") # путь к директории, которую необходимо отслеживать\n",
    "  .schema(userSchema)\n",
    "  .load()\n",
    "  .withColumn(\"Name\", get_company()) # добавим к колонкам ещё название компании\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d7d256-09ec-42b0-a38d-e15e4f4cabdf",
   "metadata": {},
   "source": [
    "Задаём логику обработки данных. Производим агргацию данных об акциях компания, где будет указана самая высокая цена акции в некотором году."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bab1c10-2d0f-4452-913f-1b264cc7da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = (\n",
    "    initDF\n",
    "    .groupBy(F.col(\"Name\"), F.year(F.col(\"Date\")).alias(\"Year\"))\n",
    "    .agg(F.max(\"High\").alias(\"Max\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ed099-dfbc-477d-8562-04985c36f93b",
   "metadata": {},
   "source": [
    "Задаём логику записи данных в консоль."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3e9032f-8f02-4d59-9aee-2665dd3b771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 00:17:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4b7d1092-3e3d-4470-9388-15f272a746ec. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/02 00:17:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "  stock_df\n",
    "  .writeStream\n",
    "  .outputMode(\"update\") # по какой логику делаем вывод - только обновления\n",
    "  .option(\"truncate\", False)\n",
    "  .format(\"console\") \n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59abe9-4263-4c74-ba3a-52361031be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372770c-b6e2-4631-a681-13a1598716fb",
   "metadata": {},
   "source": [
    "Можно записывать обработанные данные также файл и в файл, но тут есть ограничения на производимые операции. Так как запись в файл поддерживает только тип записи `append` - данные внутри файлов можно добавлять, но не изменять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b839a1-1cf0-4645-84ca-c4857b552837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 06:56:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/12/02 06:56:35 WARN StreamingQueryManager: Stopping existing streaming query [id=35be1d0b-031e-451b-bd48-0d58b19476c4, runId=382e6bed-abf5-4cf3-9dc1-0ae6b7216774], as a new run is being started.\n"
     ]
    }
   ],
   "source": [
    "stock_df = (\n",
    "    initDF\n",
    "    .where(F.col(\"Close\") - F.col(\"Open\") > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca01fdb6-6204-4470-a818-9a95a6ad14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df\\\n",
    "  .writeStream\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .format(\"csv\")\\\n",
    "  .option(\"path\", \"output\")\\\n",
    "  .option(\"header\", True)\\\n",
    "  .option(\"checkpointLocation\", \"checkpoints/\")\\\n",
    "  .start()\\\n",
    "  .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab9193-c3de-490d-9d6b-adb6e12ce69a",
   "metadata": {},
   "source": [
    "### Join & Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9351c61-444f-4d50-9468-59b76c255cdc",
   "metadata": {},
   "source": [
    "Потоковые данные можно джойнить между собой или же между привычными  batch-датафреймами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69bbf39b-30f4-4c71-b504-c6711fc09e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|Name|FullName|\n",
      "+----+--------+\n",
      "|AAPL|   Apple|\n",
      "|AMZN|  Amazon|\n",
      "+----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "companyFullNames = spark.createDataFrame(\n",
    "    [\n",
    "        (\"AAPL\", \"Apple\"),\n",
    "        (\"AMZN\", \"Amazon\"),\n",
    "        (\"GOOGL\",\"Google\"),\n",
    "        (\"MSFT\", \"Microsoft\"),\n",
    "        (\"CSCO\", \"CISCO\")\n",
    "    ], [\"Name\", \"FullName\"]\n",
    ")\n",
    "\n",
    "companyFullNames.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f320f66-663f-4f17-921a-559929c3eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = (\n",
    "    initDF\n",
    "    .groupBy(\"Name\", F.year(\"Date\").alias(\"Year\"))\n",
    "    .agg(F.max(\"High\").alias(\"Max\"))\n",
    ")\n",
    "\n",
    "stock_df = stock_df.join(companyFullNames, on=\"Name\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bca83f2b-d5f9-4ffa-bc71-7ca2e01b5490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 08:24:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5d4b566b-e312-429a-a4e3-2002f1f38d20. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/02 08:24:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+----+------+--------+\n",
      "|Name|Year|Max   |FullName|\n",
      "+----+----+------+--------+\n",
      "|AAPL|2007|28.99 |Apple   |\n",
      "|AAPL|2010|46.67 |Apple   |\n",
      "|AAPL|2013|82.16 |Apple   |\n",
      "|AAPL|2015|134.54|Apple   |\n",
      "|AAPL|2016|118.69|Apple   |\n",
      "|AAPL|2011|60.96 |Apple   |\n",
      "|AAPL|2009|30.56 |Apple   |\n",
      "|AAPL|2012|100.72|Apple   |\n",
      "|AAPL|2008|28.61 |Apple   |\n",
      "|AAPL|2006|13.31 |Apple   |\n",
      "+----+----+------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/evgeniy/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/evgeniy/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43mstock_df\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1 minute\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumRows\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stock_df\\\n",
    ".writeStream\\\n",
    ".outputMode(\"complete\")\\\n",
    ".trigger(processingTime='1 minute') \\\n",
    ".option(\"truncate\", False)\\\n",
    ".option(\"numRows\", 10)\\\n",
    ".format(\"console\")\\\n",
    ".start()\\\n",
    ".awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4ed94-1d55-499e-b470-cf2a22e81c1e",
   "metadata": {},
   "source": [
    "Теперь попробуем создать оконную функцию, в которой будем обрабатывать данные, которые находятся в одном временном окне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688e26c-5132-4284-abce-04058673ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 08:30:13 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "23/12/02 08:30:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8aa4f919-d12b-44a1-9993-0d6135795916. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/02 08:30:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----+-----+\n",
      "|window|word|count|\n",
      "+------+----+-----+\n",
      "+------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-----+-----+\n",
      "|window                                    |word |count|\n",
      "+------------------------------------------+-----+-----+\n",
      "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|world|1    |\n",
      "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|hello|1    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|hello|1    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|world|1    |\n",
      "+------------------------------------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+-----+-----+\n",
      "|window                                    |word |count|\n",
      "+------------------------------------------+-----+-----+\n",
      "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|world|1    |\n",
      "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|hello|1    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|hello|2    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|boys |1    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|world|1    |\n",
      "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|hello|1    |\n",
      "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|boys |1    |\n",
      "+------------------------------------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |word     |count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|world    |1    |\n",
      "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|hello    |1    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|hello    |2    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|boys     |1    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|world    |1    |\n",
      "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|hello    |1    |\n",
      "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|boys     |1    |\n",
      "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|words    |1    |\n",
      "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|something|1    |\n",
      "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|         |1    |\n",
      "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|         |1    |\n",
      "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|something|1    |\n",
      "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|words    |1    |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------------------------------------------+---------+-----+\n",
      "|window                                    |word     |count|\n",
      "+------------------------------------------+---------+-----+\n",
      "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|world    |1    |\n",
      "|{2023-12-02 08:29:30, 2023-12-02 08:30:30}|hello    |1    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|hello    |2    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|boys     |1    |\n",
      "|{2023-12-02 08:30:00, 2023-12-02 08:31:00}|world    |1    |\n",
      "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|hello    |1    |\n",
      "|{2023-12-02 08:30:30, 2023-12-02 08:31:30}|boys     |1    |\n",
      "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|words    |1    |\n",
      "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|something|1    |\n",
      "|{2023-12-02 08:31:00, 2023-12-02 08:32:00}|         |1    |\n",
      "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|         |1    |\n",
      "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|something|1    |\n",
      "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|words    |1    |\n",
      "|{2023-12-02 08:31:30, 2023-12-02 08:32:30}|world    |1    |\n",
      "|{2023-12-02 08:32:00, 2023-12-02 08:33:00}|world    |1    |\n",
      "+------------------------------------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format('socket')\n",
    "    .option('host', 'localhost')\n",
    "    .option('port', '10000')\n",
    "    .option('includeTimestamp', 'true')\n",
    "    .load()\n",
    ")\n",
    "\n",
    "words = lines.select(\n",
    "    F.explode(F.split(lines.value, ' ')).alias('word'),\n",
    "    lines.timestamp\n",
    ")\n",
    "\n",
    "windowDuration = '1 minutes'\n",
    "slideDuration = '30 seconds'\n",
    "\n",
    "windowedCounts = words.groupBy(\n",
    "    F.window(words.timestamp, windowDuration, slideDuration),\n",
    "    words.word\n",
    ").count().orderBy('window')\n",
    "\n",
    "# Start running the query that prints the windowed word counts to the console\n",
    "query = (windowedCounts\n",
    "    .writeStream\n",
    "    .outputMode('complete')\n",
    "    .format('console')\n",
    "    .option('truncate', 'false')\n",
    "    .start()\n",
    ")\n",
    "         \n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2cf39-e571-48af-844b-66f1b7c4b62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
