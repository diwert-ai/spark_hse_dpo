{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Промышленное машинное обучение на Spark`\n",
    "## `Занятие 03: Основы Spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О чём можно узнать из этого ноутбука:\n",
    "\n",
    "* DataFrame и SQL API\n",
    "* Базовые операции в Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxEu0kipR2jE"
   },
   "source": [
    "### `Устанавливаем необходимые зависимости`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:07.761968Z",
     "start_time": "2023-05-03T03:55:03.004378Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTs7dYpxO6pF",
    "outputId": "0bd5516e-afb7-40b0-a0b9-fd0ead01470e"
   },
   "outputs": [],
   "source": [
    "! pip3 install -q pyspark pyarrow parquet-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Готовим SparkContext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как следует из лекции объект SparkContext является точкой входа для работы со Spark кластером. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:16.396171Z",
     "start_time": "2023-05-03T03:55:07.766558Z"
    },
    "id": "JTjLPr5jFhpR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/20 22:03:48 WARN Utils: Your hostname, vm-01 resolves to a loopback address: 127.0.1.1; using 10.128.0.16 instead (on interface eth0)\n",
      "23/10/20 22:03:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/20 22:03:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Создаём конфигурационный класс с параметрами подключения\n",
    "conf = (\n",
    "    SparkConf()\n",
    "        # Указываем порт на котором будет располагаться UI\n",
    "        .set('spark.ui.port', '4050')\n",
    "        # Указываем URL master ноды Spark кластера\n",
    "        # Можно использовать local mode, указав `local[<number_cores>]`\n",
    "        # В таком случае вся обработка будет происходить на текущем компьютере\n",
    "        # При этом, это может давать преимущество ввиду наличия параллелизма по ядрам компьютера\n",
    "        .setMaster('local[*]')\n",
    "        # Если нужно подключиться к \"реальному\" кластеру то нужно указать URL `spark://<master-node-url:master-node-url>`. Например:\n",
    "        # .setMaster('spark://localhost:7077')\n",
    ")\n",
    "# Создаём точку доступа на кластер. Позволят использовать RDD API\n",
    "sc = SparkContext(conf=conf)\n",
    "# Точка доступа для использования DataFrame API\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# По завершении программы нужно обязательно выполнить остановку подключения для освобождения занятых ресурсов\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После запуска spark у нас доступен UI интерфейс, где будет отображаться информация о Spark-кластере, запущенных задачах и выводы различных логов.\n",
    "\n",
    "Чтобы перейти в UI интерфейс достаточно в новом окне браузера ввести: <Адрес вашей виртуальной машины>:4050/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SOyISvrSIun"
   },
   "source": [
    "### `Загрузка данных`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMohSEhKSN0R"
   },
   "source": [
    "Скачаем данные, с которыми будем в дальнейшем работать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:18.912291Z",
     "start_time": "2023-05-03T03:55:16.399069Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLhF993mGCQx",
    "outputId": "1b9ae695-e4f1-4eaa-f03e-0fba9ae93c45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://downloader.disk.yandex.ru/disk/4ac806482fc654005c797b84b072642fd6b642ebba0d79028e71bdb2acbb3962/65333a7e/qo_oUU0UNiEpD-z6-zCIgdRB33v8cVyWATClBo2FTm36Wiufn6dFhyr1IoTkBcuIDOqKvX5GKvyJZKr2SKucCw%3D%3D?uid=0&filename=m5-forecating-accuracy.zip&disposition=attachment&hash=J5ovyMT7FWTmxkOjFLIMj3wuXKK82PqrkEXM2lsA3isVM3La/mWxkoo43Q/uGJZ5q/J6bpmRyOJonT3VoXnDag%3D%3D%3A/m5-forecating-accuracy.zip&limit=0&content_type=application%2Fzip&owner_uid=1199758960&fsize=48326531&hid=d3329dd040d2a0b0116f34e36f7142dc&media_type=compressed&tknv=v2'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "folder_url = 'https://disk.yandex.lt/d/JnDy1h48pJI7IA'\n",
    "file_url = '/m5-forecating-accuracy.zip'\n",
    "# запрос ссылки на скачивание\n",
    "response = requests.get('https://cloud-api.yandex.net/v1/disk/public/resources/download',\n",
    "                 params={'public_key': folder_url, 'path': file_url}) \n",
    "# 'парсинг' ссылки на скачивание\n",
    "data_link = response.json()['href'] \t\n",
    "data_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте, данные по ссылке, которую вы получите после запуска предыдущей ячейки.\n",
    "\n",
    "Для скачивания необходимо ввести команду: wget -O m5-forecating-accuracy.zip \\<YOUR_DATA_LINK\\>\n",
    "\n",
    "P.S.: Ссылка с github может быть уже недействительной, поэтому нужно обязательно запустить у себя ячейку выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-20 22:41:47--  https://downloader.disk.yandex.ru/disk/2bfe1796ad8fef44ceb25f1b8332c7d99d05c5b1561b238bf21dedf38bc9edd5/65333993/qo_oUU0UNiEpD-z6-zCIgdRB33v8cVyWATClBo2FTm36Wiufn6dFhyr1IoTkBcuIDOqKvX5GKvyJZKr2SKucCw%3D%3D?uid=0&filename=m5-forecating-accuracy.zip&disposition=attachment&hash=J5ovyMT7FWTmxkOjFLIMj3wuXKK82PqrkEXM2lsA3isVM3La/mWxkoo43Q/uGJZ5q/J6bpmRyOJonT3VoXnDag%3D%3D%3A/m5-forecating-accuracy.zip&limit=0&content_type=application%2Fzip&owner_uid=1199758960&fsize=48326531&hid=d3329dd040d2a0b0116f34e36f7142dc&media_type=compressed&tknv=v2\n",
      "Resolving downloader.disk.yandex.ru (downloader.disk.yandex.ru)... 77.88.21.127, 2a02:6b8::2:127\n",
      "Connecting to downloader.disk.yandex.ru (downloader.disk.yandex.ru)|77.88.21.127|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://s235vla.storage.yandex.net/rdisk/2bfe1796ad8fef44ceb25f1b8332c7d99d05c5b1561b238bf21dedf38bc9edd5/65333993/qo_oUU0UNiEpD-z6-zCIgdRB33v8cVyWATClBo2FTm36Wiufn6dFhyr1IoTkBcuIDOqKvX5GKvyJZKr2SKucCw==?uid=0&filename=m5-forecating-accuracy.zip&disposition=attachment&hash=J5ovyMT7FWTmxkOjFLIMj3wuXKK82PqrkEXM2lsA3isVM3La/mWxkoo43Q/uGJZ5q/J6bpmRyOJonT3VoXnDag%3D%3D%3A/m5-forecating-accuracy.zip&limit=0&content_type=application%2Fzip&owner_uid=1199758960&fsize=48326531&hid=d3329dd040d2a0b0116f34e36f7142dc&media_type=compressed&tknv=v2&rtoken=HfDBicMiWWFK&force_default=no&ycrid=na-e0ffdf55c3bae952693e438b6b966edb-downloader3h&ts=60830e1434ac0&s=35334a5af85fefe2998823befcca9935c0262e132d16aac422de69a361e78656&pb=U2FsdGVkX19b62F_raP-4x85Z2Y8B1onQ2QObvc6Vv8tBglFbe1odfyOuwhRNmAyEIOWquk_kDTpOEl7ADxjobyyXUzHb4DRlF0DHg5XA4o [following]\n",
      "--2023-10-20 22:41:47--  https://s235vla.storage.yandex.net/rdisk/2bfe1796ad8fef44ceb25f1b8332c7d99d05c5b1561b238bf21dedf38bc9edd5/65333993/qo_oUU0UNiEpD-z6-zCIgdRB33v8cVyWATClBo2FTm36Wiufn6dFhyr1IoTkBcuIDOqKvX5GKvyJZKr2SKucCw==?uid=0&filename=m5-forecating-accuracy.zip&disposition=attachment&hash=J5ovyMT7FWTmxkOjFLIMj3wuXKK82PqrkEXM2lsA3isVM3La/mWxkoo43Q/uGJZ5q/J6bpmRyOJonT3VoXnDag%3D%3D%3A/m5-forecating-accuracy.zip&limit=0&content_type=application%2Fzip&owner_uid=1199758960&fsize=48326531&hid=d3329dd040d2a0b0116f34e36f7142dc&media_type=compressed&tknv=v2&rtoken=HfDBicMiWWFK&force_default=no&ycrid=na-e0ffdf55c3bae952693e438b6b966edb-downloader3h&ts=60830e1434ac0&s=35334a5af85fefe2998823befcca9935c0262e132d16aac422de69a361e78656&pb=U2FsdGVkX19b62F_raP-4x85Z2Y8B1onQ2QObvc6Vv8tBglFbe1odfyOuwhRNmAyEIOWquk_kDTpOEl7ADxjobyyXUzHb4DRlF0DHg5XA4o\n",
      "Resolving s235vla.storage.yandex.net (s235vla.storage.yandex.net)... 77.88.33.172, 2a02:6b8:c0e:920:0:41af:a440:fa7\n",
      "Connecting to s235vla.storage.yandex.net (s235vla.storage.yandex.net)|77.88.33.172|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 48326531 (46M) [application/zip]\n",
      "Saving to: ‘m5-forecasting-accuracy.zip’\n",
      "\n",
      "m5-forecasting-accu 100%[===================>]  46.09M   167MB/s    in 0.3s    \n",
      "\n",
      "2023-10-20 22:41:47 (167 MB/s) - ‘m5-forecasting-accuracy.zip’ saved [48326531/48326531]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Вставьте здесь свой data_link\n",
    "! wget -O m5-forecasting-accuracy.zip 'https://downloader.disk.yandex.ru/disk/2bfe1796ad8fef44ceb25f1b8332c7d99d05c5b1561b238bf21dedf38bc9edd5/65333993/qo_oUU0UNiEpD-z6-zCIgdRB33v8cVyWATClBo2FTm36Wiufn6dFhyr1IoTkBcuIDOqKvX5GKvyJZKr2SKucCw%3D%3D?uid=0&filename=m5-forecating-accuracy.zip&disposition=attachment&hash=J5ovyMT7FWTmxkOjFLIMj3wuXKK82PqrkEXM2lsA3isVM3La/mWxkoo43Q/uGJZ5q/J6bpmRyOJonT3VoXnDag%3D%3D%3A/m5-forecating-accuracy.zip&limit=0&content_type=application%2Fzip&owner_uid=1199758960&fsize=48326531&hid=d3329dd040d2a0b0116f34e36f7142dc&media_type=compressed&tknv=v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMohSEhKSN0R"
   },
   "source": [
    "В этом проекте нужно работать с данными для предсказания спроса: [M5 Forecasting](https://www.kaggle.com/competitions/m5-forecasting-accuracy/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:18.921412Z",
     "start_time": "2023-05-03T03:55:18.917244Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './m5-forecasting-accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:24.526445Z",
     "start_time": "2023-05-03T03:55:18.925305Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLhF993mGCQx",
    "outputId": "1b9ae695-e4f1-4eaa-f03e-0fba9ae93c45"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('./m5-forecasting-accuracy.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./m5-forecasting-accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:25.408666Z",
     "start_time": "2023-05-03T03:55:24.529366Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jvOMphiFwRi",
    "outputId": "ab014389-903e-4e9d-8967-ba3d203334ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar.csv  sales_train_evaluation.csv  sample_submission.csv\n",
      "\u001b[0m\u001b[01;34m__MACOSX\u001b[0m/     sales_train_validation.csv  sell_prices.csv\n"
     ]
    }
   ],
   "source": [
    "%ls $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:42.286781Z",
     "start_time": "2023-05-03T03:55:25.412229Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "0aoAjM9VGUQC",
    "outputId": "5bb02e2e-ea31-4470-f2b0-290754056740"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/20 22:46:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_006_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_006</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_007_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_007</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_010</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "5  HOBBIES_1_006_CA_1_validation  HOBBIES_1_006  HOBBIES_1  HOBBIES     CA_1   \n",
       "6  HOBBIES_1_007_CA_1_validation  HOBBIES_1_007  HOBBIES_1  HOBBIES     CA_1   \n",
       "7  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1   \n",
       "8  HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES     CA_1   \n",
       "9  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
       "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
       "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
       "5       CA    0    0    0    0  ...       0       1       0       1       0   \n",
       "6       CA    0    0    0    0  ...       0       0       0       1       0   \n",
       "7       CA   12   15    0    0  ...       0       0       1      37       3   \n",
       "8       CA    2    0    7    3  ...       0       0       1       1       6   \n",
       "9       CA    0    0    1    0  ...       1       0       0       0       0   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "2       1       0       1       1       1  \n",
       "3       0       1       3       7       2  \n",
       "4       1       2       2       2       4  \n",
       "5       0       0       2       0       0  \n",
       "6       1       0       0       1       1  \n",
       "7       4       6       3       2       1  \n",
       "8       0       0       0       0       0  \n",
       "9       0       0       2       0       2  \n",
       "\n",
       "[10 rows x 1919 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Зададим пути к файлам из датасета\n",
    "file_calendar = f\"{path}/calendar.csv\"\n",
    "file_validation = f\"{path}/sales_train_validation.csv\"\n",
    "file_evaluation = f\"{path}/sales_train_evaluation.csv\"\n",
    "file_prices = f\"{path}/sell_prices.csv\"\n",
    "\n",
    "# Формат данных — CSV\n",
    "file_type = \"csv\"\n",
    "# Зададим параметры, как интерпретировать загруженные данные\n",
    "# Определять типы колонок автоматически\n",
    "infer_schema = \"true\"\n",
    "# Интерпретируем первую строку в файле, как названия колонок\n",
    "first_row_is_header = \"true\"\n",
    "# Задаём разделитель между значениями колонок\n",
    "delimiter = \",\"\n",
    "\n",
    "# загружаем в Spark данные валидации\n",
    "df_validation = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_validation)\n",
    "    # Также, можно указывать пути в hdfs или базы данных, например, Hive\n",
    "#       .load('hdfs:///path_to_data/...')\n",
    ")\n",
    "\n",
    "# загружаем данные для теста\n",
    "df_evaluation = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_evaluation)\n",
    ")\n",
    "# загружаем данные с ценами\n",
    "df_prices = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_prices)\n",
    ")\n",
    "\n",
    "# Возьмём первые 10 строк pyspark.sql.dataframe.DataFrame\n",
    "# И выполним action-преобразование для преобразования в pandas.DataFrame\n",
    "df_validation.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCsO86XSW9kW"
   },
   "source": [
    "### `Spark DataFrame API`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fw_vMfqYd2b"
   },
   "source": [
    "* [Quickstart](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html)\n",
    "* [Документация](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:42.365772Z",
     "start_time": "2023-05-03T03:55:42.289608Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCbwFDIGXEcA",
    "outputId": "c27b98b7-f064-4897-e2cb-57ebc1f832a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_id: bigint, name: string, dept_id: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_data = [\n",
    "    (1, 'Smith', 10),\n",
    "    (2, 'Rose', 20),\n",
    "    (3, 'Williams', 10),\n",
    "    (4, 'Jones', 30),\n",
    "    (5, 'Jones', None),\n",
    "]\n",
    "emp_columns = ['emp_id', 'name', 'dept_id']\n",
    "\n",
    "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
    "emp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:42.373636Z",
     "start_time": "2023-05-03T03:55:42.368591Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOdJOWisX_Aj",
    "outputId": "c6dd0501-3a49-4f07-fcf4-db392553ed6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выведем тип датафрейма\n",
    "type(emp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем посмотреть, что находится в emp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_id: bigint, name: string, dept_id: bigint]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод DataFrame не показывает его содержимое, так как оно ещё не было вычислено, вычисления в Spark происходят только в момент вызова функций типа action.\n",
    "\n",
    "Примеры action:\n",
    "* `count()` — подсчитывает число строк в DataFrame\n",
    "* `toPandas()` — преобразует Spark DataFrame в pandas DataFrame\n",
    "* `collect()` — выполняет вычисление текущего Spark DataFrame и возвращает результат\n",
    "* `show()` — `collect()` + pretty print результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:43.700269Z",
     "start_time": "2023-05-03T03:55:42.381922Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|emp_id|    name|dept_id|\n",
      "+------+--------+-------+\n",
      "|     1|   Smith|     10|\n",
      "|     2|    Rose|     20|\n",
      "|     3|Williams|     10|\n",
      "|     4|   Jones|     30|\n",
      "|     5|   Jones|   NULL|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим базовую информация о данных — названия колонок и тип данных, хранящийся в них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:43.708843Z",
     "start_time": "2023-05-03T03:55:43.703778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emp_id', 'name', 'dept_id']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('emp_id', LongType(), True), StructField('name', StringType(), True), StructField('dept_id', LongType(), True)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame.columns - показывает названия колонок внутри spark-dataframe\n",
    "print(emp_df.columns)\n",
    "# DataFrame.schema - показывает их типы\n",
    "emp_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие методы дублируются по аналогии с `pandas.DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:44.283929Z",
     "start_time": "2023-05-03T03:55:43.712912Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hhTSzDDX5zWw",
    "outputId": "8cd5a932-fefe-4b1c-ce40-615a057e1269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|emp_id|    name|dept_id|\n",
      "+------+--------+-------+\n",
      "|     1|   Smith|     10|\n",
      "|     2|    Rose|     20|\n",
      "|     3|Williams|     10|\n",
      "|     4|   Jones|     30|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Уберём строки, содержащие хотя бы одно NULL значение\n",
    "emp_df.dropna().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame состоит из колонок. Получение колонки возможно через атрибуты (точечная нотация) или через индексацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:44.300002Z",
     "start_time": "2023-05-03T03:55:44.287375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<'name'>, Column<'name'>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.name, emp_df['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо самих значений колонок, в соответствии с принципом \"ленивых\" вычислений, возвращаются ссылки на них. Такие колонки могут участвовать в символьных вычислениях. Например, к ним можно применять арифметические, булевы операции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:44.320080Z",
     "start_time": "2023-05-03T03:55:44.303113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(((dept_id - 20) / 10) > emp_id)'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cозданим новую колонку с данными, на основе данных из колонок dep_id и emp_id,\n",
    "# в соответствии с операцийми ниже\n",
    "column_expr = (emp_df.dept_id - 20) / 10 > emp_df.emp_id\n",
    "column_expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как не было выполненно ни одной операции тип action c новой колонкой, при выводе выдаётся только её символическая запись. Чтобы исправить это, запустим операцию типа-action show(), которая делает вывод посчитанного результата."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные **колоночные выражения** (**column expression**) можно вычислять:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:44.805817Z",
     "start_time": "2023-05-03T03:55:44.323998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|(((dept_id - 20) / 10) > emp_id)|\n",
      "+--------------------------------+\n",
      "|                           false|\n",
      "|                           false|\n",
      "|                           false|\n",
      "|                           false|\n",
      "|                            NULL|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(column_expr).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Колонку можно переименовать, используя метод .alias(\\<new_name\\>) для данной колонки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:45.309115Z",
     "start_time": "2023-05-03T03:55:44.810516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dept_id squared|\n",
      "+---------------+\n",
      "|          100.0|\n",
      "|          400.0|\n",
      "|          100.0|\n",
      "|          900.0|\n",
      "|           NULL|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select((emp_df.dept_id ** 2).alias('dept_id squared')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для DataFrame доступны SQL подобные операции, например, `join`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:45.758201Z",
     "start_time": "2023-05-03T03:55:45.311701Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpatCy-5XsvD",
    "outputId": "37be9337-77ec-41ee-9cd0-4c298fe6e503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_data = [\n",
    "    ('Finance', 10),\n",
    "    ('Marketing', 20),\n",
    "    ('Sales', 30),\n",
    "    ('IT', 40),\n",
    "]\n",
    "dept_columns = ['dept_name', 'dept_id']\n",
    "\n",
    "# создаём spark-датафрейм\n",
    "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:46.982880Z",
     "start_time": "2023-05-03T03:55:45.760896Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhU1JequYEvz",
    "outputId": "d975a439-0c5a-41ce-83c5-ed31e8c64bca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+---------+\n",
      "|dept_id|emp_id|    name|dept_name|\n",
      "+-------+------+--------+---------+\n",
      "|     10|     1|   Smith|  Finance|\n",
      "|     10|     3|Williams|  Finance|\n",
      "|     20|     2|    Rose|Marketing|\n",
      "|     30|     4|   Jones|    Sales|\n",
      "+-------+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# через метод .join() объеденим emp_df с dept_df по ключю dept_id\n",
    "emp_df.join(dept_df, how='inner', on=['dept_id']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущем примере объединение таблиц происходило только по пересекающимся (параметр how='inner') значениям dept_id, теперь объденим таблицы по всевозможным ключам (параметр how='outer')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:47.717944Z",
     "start_time": "2023-05-03T03:55:46.989769Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AmpYXEEZVcg",
    "outputId": "a4c37de4-5b37-4c69-ad7c-accecf6892cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+---------+\n",
      "|dept_id|emp_id|    name|dept_name|\n",
      "+-------+------+--------+---------+\n",
      "|   NULL|     5|   Jones|     NULL|\n",
      "|     10|     1|   Smith|  Finance|\n",
      "|     10|     3|Williams|  Finance|\n",
      "|     20|     2|    Rose|Marketing|\n",
      "|     30|     4|   Jones|    Sales|\n",
      "|     40|  NULL|    NULL|       IT|\n",
      "+-------+------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, how='outer', on=['dept_id']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, доступна фильтрация и сортировка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:48.557959Z",
     "start_time": "2023-05-03T03:55:47.723042Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddeQGejsaFfV",
    "outputId": "73b5fa01-5fcd-42f7-9ed9-ee94c774b0b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:===============================================================================>                                                                               (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+---------+\n",
      "|dept_id|emp_id| name|dept_name|\n",
      "+-------+------+-----+---------+\n",
      "|     10|     1|Smith|  Finance|\n",
      "|     20|     2| Rose|Marketing|\n",
      "+-------+------+-----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "# метод where отвечает за фильрации, внутри которого указываются условия фильтрации\n",
    "# метод sort производит сортировку по полю/полям, которые указаны в его аргументах\n",
    "(\n",
    "    emp_df\n",
    "      .join(dept_df, how='outer', on=['dept_id'])\n",
    "      # Обратите внимание на колоночное выражение в фильтре\n",
    "      .where((emp_df['name'] == 'Smith') | (emp_df['name'] == 'Rose'))\n",
    "      .sort('dept_id')\n",
    "      .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с колонками обычно выполняется через колоночные выражения. Их можно использовать, например, для выполнения join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:49.022712Z",
     "start_time": "2023-05-03T03:55:48.562014Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfHsc6tvale3",
    "outputId": "2d26ce94-5c4c-49d8-f4a2-e520524d5a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|emp_id|    name|emp_dept_id|\n",
      "+------+--------+-----------+\n",
      "|     1|   Smith|         10|\n",
      "|     2|    Rose|         20|\n",
      "|     3|Williams|         10|\n",
      "|     4|   Jones|         30|\n",
      "|     5|   Jones|       NULL|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_columns_renamed = ['emp_id', 'name', 'emp_dept_id']\n",
    "\n",
    "emp_renamed_df = spark.createDataFrame(emp_data, emp_columns_renamed)\n",
    "emp_renamed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:49.679335Z",
     "start_time": "2023-05-03T03:55:49.025389Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mM2ieylzbWlj",
    "outputId": "2547f5c2-77f7-4618-e3a4-fe84005d2fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|\n",
      "+------+--------+-----------+---------+-------+\n",
      "|     1|   Smith|         10|  Finance|     10|\n",
      "|     3|Williams|         10|  Finance|     10|\n",
      "|     2|    Rose|         20|Marketing|     20|\n",
      "|     4|   Jones|         30|    Sales|     30|\n",
      "+------+--------+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# теперь вместо параметра  on=['dept_id'] в методе join используем колоночное выражение\n",
    "emp_renamed_df.join(\n",
    "    dept_df, emp_renamed_df.emp_dept_id == dept_df.dept_id,  how='inner'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переименование колонок также возможно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:50.343395Z",
     "start_time": "2023-05-03T03:55:49.682852Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDhzSDSsbhY3",
    "outputId": "c8c69547-d309-44c7-ce74-6cb8d017589b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:===============================================================================>                                                                               (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+---------+\n",
      "|dept_id|emp_id|    name|dept_name|\n",
      "+-------+------+--------+---------+\n",
      "|     10|     1|   Smith|  Finance|\n",
      "|     10|     3|Williams|  Finance|\n",
      "|     20|     2|    Rose|Marketing|\n",
      "|     30|     4|   Jones|    Sales|\n",
      "+-------+------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "# .withColumnRenamed(<old_name>, <new_name>)\n",
    "(\n",
    "    emp_renamed_df\n",
    "      .withColumnRenamed('emp_dept_id', 'dept_id')\n",
    "      .join(\n",
    "          dept_df, 'dept_id',  how='inner'\n",
    "      )\n",
    "      .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для преобразования колонок в модуле `pyspark.sql.functions` содержится большой набор вспомогательных функций. Например:\n",
    "* Вспомогательные: `lit`, `col`, ...\n",
    "* Поэлементные математические функции: `cos`, `sin`, `round`, ...\n",
    "* Поэлементные функции для работы датами и временем: `dayofmonth`, ...\n",
    "* Агрегаторы: `sum`, `mean`, ...\n",
    "* Функции для работы с коллекциями (сложными данными, хранящимся в колонке): `array_sort`, `concat`, ...\n",
    "* Сортировки: `asc`, ...\n",
    "* Строковые функции: `concat_ws`, `lower`, `split`, ...\n",
    "* Оконные функции: `lag`, ...\n",
    "* Преобразования с пользовательскими функциями: `udf_pandas`, ...\n",
    "\n",
    "**Всегда перед написанием кода нужно подумать, нет ли уже готовой функции в данном модуле. Использование готовых функции существенно влияет на скорость вычислений.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:50.351376Z",
     "start_time": "2023-05-03T03:55:50.347205Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для применения функций нужно поменять тип колонки, за что отвечает метод .cast(\\<NEW_COLUMN_TYPE\\>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:50.976103Z",
     "start_time": "2023-05-03T03:55:50.354861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+----------+\n",
      "|emp_id|    name|dept_id| hire_date|\n",
      "+------+--------+-------+----------+\n",
      "|     1|   Smith|     10|2000-01-10|\n",
      "|     2|    Rose|     20|2010-02-20|\n",
      "|     3|Williams|     10|2000-03-10|\n",
      "|     4|   Jones|     30|2020-04-30|\n",
      "+------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_with_date = (\n",
    "    emp_df\n",
    "        .dropna()\n",
    "        .withColumn(\n",
    "            'hire_date', \n",
    "            # Конструируем дату в формате yyyy-mm-dd \n",
    "            F.concat_ws(\n",
    "                '-',\n",
    "                # Придумываем год\n",
    "                (1990 + emp_df.dept_id).cast(StringType()),\n",
    "                # Придумываем месяц\n",
    "                F.concat(F.lit('0'), emp_df.emp_id.cast(StringType())), \n",
    "                # Придумываем день\n",
    "                emp_df.dept_id.cast(StringType())\n",
    "            ).cast(DateType())\n",
    "        )\n",
    ")\n",
    "emp_with_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:51.440025Z",
     "start_time": "2023-05-03T03:55:50.979677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------+\n",
      "|ACOS((emp_id / 4))|year(hire_date)|processed_name|\n",
      "+------------------+---------------+--------------+\n",
      "| 1.318116071652818|           2000|          cмит|\n",
      "|1.0471975511965979|           2010|          rose|\n",
      "|0.7227342478134157|           2000|      williams|\n",
      "|               0.0|           2020|         jones|\n",
      "+------------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_with_date.select(\n",
    "    # вызваем arccos для колонки\n",
    "    F.acos(emp_with_date.emp_id / 4),\n",
    "    # берём от колонки с датой только значение года\n",
    "    F.year(emp_with_date.hire_date), \n",
    "    # заменяем все вхождения smith на смит и переименовываем её в processed_name\n",
    "    F.regexp_replace(F.lower(emp_with_date.name), 'smith', 'cмит').alias('processed_name') \n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO38c_qmcYCT"
   },
   "source": [
    "### `Spark SQL API`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с таблицами в Spark возможна не только в Pandas-подобном интерфейсе, также поддерживается SQL выражения для обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:51.857953Z",
     "start_time": "2023-05-03T03:55:51.443880Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdpoa0q-crM4",
    "outputId": "af4c7c0a-5bc4-4c60-d954-c9e3d89b964d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+--------+-----+\n",
      "|emp_id|        address|    city|state|\n",
      "+------+---------------+--------+-----+\n",
      "|     1|   1523 Main St|     SFO|   CA|\n",
      "|     2| 3453 Orange St|     SFO|   NY|\n",
      "|     3|   34 Warner St|  Jersey|   NJ|\n",
      "|     4|221 Cavalier St|  Newark|   DE|\n",
      "|     5|  789 Walnut St|Sandiago|   CA|\n",
      "+------+---------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_data = [\n",
    "    (1, '1523 Main St', 'SFO', 'CA'),\n",
    "    (2, '3453 Orange St', 'SFO', 'NY'),\n",
    "    (3, '34 Warner St', 'Jersey', 'NJ'),\n",
    "    (4, '221 Cavalier St', 'Newark', 'DE'),\n",
    "    (5, '789 Walnut St', 'Sandiago', 'CA')\n",
    "]\n",
    "add_columns = ['emp_id', 'address', 'city', 'state']\n",
    "# создадим тестовую таблицу\n",
    "add_df = spark.createDataFrame(add_data, add_columns)\n",
    "add_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark позволяет использовать DataFrame в качестве таблиц в регулярных SQL запросах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:51.938821Z",
     "start_time": "2023-05-03T03:55:51.869856Z"
    },
    "id": "8QoIUY3mcLOm"
   },
   "outputs": [],
   "source": [
    "# создаём таблицу с именем EMP на основе датафрейма emp_df\n",
    "emp_df.createOrReplaceTempView('EMP')\n",
    "# создаём таблицу с именем DEPT на основе датафрейма dept_df\n",
    "dept_df.createOrReplaceTempView('DEPT')\n",
    "# создаём таблицу с именем DEPT на основе датафрейма add_df\n",
    "add_df.createOrReplaceTempView('ADD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:53.075199Z",
     "start_time": "2023-05-03T03:55:51.941888Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evVxaSO9dRvJ",
    "outputId": "598170d5-2b2a-4d79-ceee-1b74fdd252a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+---------+-------+------+---------------+------+-----+\n",
      "|emp_id|    name|dept_id|dept_name|dept_id|emp_id|        address|  city|state|\n",
      "+------+--------+-------+---------+-------+------+---------------+------+-----+\n",
      "|     1|   Smith|     10|  Finance|     10|     1|   1523 Main St|   SFO|   CA|\n",
      "|     2|    Rose|     20|Marketing|     20|     2| 3453 Orange St|   SFO|   NY|\n",
      "|     3|Williams|     10|  Finance|     10|     3|   34 Warner St|Jersey|   NJ|\n",
      "|     4|   Jones|     30|    Sales|     30|     4|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-------+---------+-------+------+---------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# выполняем сам SQL запрос над данными таблицами\n",
    "spark.sql('''\n",
    "    select * from EMP e, DEPT d, ADD a\n",
    "    where e.dept_id == d.dept_id and e.emp_id == a.emp_id\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybPKx2_0eJAG"
   },
   "source": [
    "### `Ещё базовые операции над Spark DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:53.520078Z",
     "start_time": "2023-05-03T03:55:53.078716Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3RTYC2LdsR7",
    "outputId": "8e8d9960-217c-4837-b77b-f8996e526521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   Name|      Dept|Salary|\n",
      "+-------+----------+------+\n",
      "|  James|     Sales|  3000|\n",
      "|Michael|     Sales|  4600|\n",
      "| Robert|     Sales|  4100|\n",
      "|  Maria|   Finance|  3000|\n",
      "|  James|     Sales|  3000|\n",
      "|  Scott|   Finance|  3300|\n",
      "|    Jen|   Finance|  3900|\n",
      "|   Jeff| Marketing|  3000|\n",
      "|  Kumar| Marketing|  2000|\n",
      "|   Saif|     Sales|  4100|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Michael', 'Sales', 4600),\n",
    "    ('Robert', 'Sales', 4100),\n",
    "    ('Maria', 'Finance', 3000),\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Scott', 'Finance', 3300),\n",
    "    ('Jen', 'Finance', 3900),\n",
    "    ('Jeff', ' Marketing', 3000),\n",
    "    ('Kumar', 'Marketing', 2000),\n",
    "    ('Saif', 'Sales', 4100),\n",
    "]\n",
    "columns = ['Name', 'Dept', 'Salary']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:53.979903Z",
     "start_time": "2023-05-03T03:55:53.523202Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYR2TObjerC1",
    "outputId": "3bbcd476-9e47-40ba-d9e0-7121978e41e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   Name|      Dept|Salary|\n",
      "+-------+----------+------+\n",
      "|Michael|     Sales|  4600|\n",
      "|  James|     Sales|  3000|\n",
      "| Robert|     Sales|  4100|\n",
      "|  Maria|   Finance|  3000|\n",
      "|    Jen|   Finance|  3900|\n",
      "|  Scott|   Finance|  3300|\n",
      "|  Kumar| Marketing|  2000|\n",
      "|   Saif|     Sales|  4100|\n",
      "|   Jeff| Marketing|  3000|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# метод .distinct отбирает только уникальные строки в датафрейме\n",
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:54.411672Z",
     "start_time": "2023-05-03T03:55:53.982570Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyZ0mIvsethd",
    "outputId": "d890ce5d-ca5e-49d1-b7b1-73d0c6a964ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# метод count() считаем число строк в датафрейме\n",
    "# так как count относится к action операциям, то метод show можно не вызывать\n",
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, возможно использовать группировку и агрегаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:54.932782Z",
     "start_time": "2023-05-03T03:55:54.415371Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ACpgoLAQe1wq",
    "outputId": "c36996e9-b54a-49b9-d875-1ad46969f677"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Dept='Sales', sum(Salary)=18800),\n",
       " Row(Dept='Finance', sum(Salary)=10200),\n",
       " Row(Dept=' Marketing', sum(Salary)=3000),\n",
       " Row(Dept='Marketing', sum(Salary)=2000)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сгрупировать данные по департаменту и посчитать суммарную зарплату всех сотрудникаов в нём\n",
    "df.groupBy('Dept').sum().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtBLc8D6hJGb"
   },
   "source": [
    "### `IO операции`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как было произведены все операции с данными нужно сохранить результаты их работы на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:54.969927Z",
     "start_time": "2023-05-03T03:55:54.936851Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnoAthLTgWhk",
    "outputId": "818089ee-fd43-4e3d-d8c2-00eb63e80c70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[min_salary: bigint, mean_salary: double, max_salary: bigint]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_statistics = df.select(\n",
    "    F.min('Salary').alias('min_salary'),\n",
    "    F.mean('Salary').alias('mean_salary'),\n",
    "    F.max('Salary').alias('max_salary')\n",
    ")\n",
    "# Пока никаких вычислений не произошло\n",
    "base_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:55:56.864684Z",
     "start_time": "2023-05-03T03:55:54.975212Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "pb8lH5i0iZ8g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "# записываем данные на диск в CSV формате формате\n",
    "base_statistics.write.csv('./base_statistics.csv', header=True)\n",
    "# записываем данные на диск в специальном spark-формате parquet\n",
    "base_statistics.write.parquet('./base_statistics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:56:10.314253Z",
     "start_time": "2023-05-03T03:56:09.473927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-cb924d66-c4d7-4a7d-9c4a-9991b308fa9a-c000.csv  _SUCCESS\n",
      "Error: no such file, variable, URL, history range or macro\n"
     ]
    }
   ],
   "source": [
    "%ls ./base_statistics.csv/\n",
    "%pycat ./base_statistics.csv/part-00000-5f1b82c2-0fe0-4305-8c71-067297fd29b2-c000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для просмотра parquet-файлов нужны специльные инструменты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:56:25.040198Z",
     "start_time": "2023-05-03T03:56:22.429642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-17fd217a-ca74-4556-bb0b-991cee8924dc-c000.snappy.parquet  _SUCCESS\n",
      "\n",
      "############ file meta data ############\n",
      "created_by: parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba)\n",
      "num_columns: 3\n",
      "num_rows: 1\n",
      "num_row_groups: 1\n",
      "format_version: 1.0\n",
      "serialized_size: 789\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "############ Columns ############\n",
      "min_salary\n",
      "mean_salary\n",
      "max_salary\n",
      "\n",
      "############ Column(min_salary) ############\n",
      "name: min_salary\n",
      "path: min_salary\n",
      "max_definition_level: 1\n",
      "max_repetition_level: 0\n",
      "physical_type: INT64\n",
      "logical_type: None\n",
      "converted_type (legacy): NONE\n",
      "compression: SNAPPY (space_saved: -5%)\n",
      "\n",
      "############ Column(mean_salary) ############\n",
      "name: mean_salary\n",
      "path: mean_salary\n",
      "max_definition_level: 1\n",
      "max_repetition_level: 0\n",
      "physical_type: DOUBLE\n",
      "logical_type: None\n",
      "converted_type (legacy): NONE\n",
      "compression: SNAPPY (space_saved: -5%)\n",
      "\n",
      "############ Column(max_salary) ############\n",
      "name: max_salary\n",
      "path: max_salary\n",
      "max_definition_level: 1\n",
      "max_repetition_level: 0\n",
      "physical_type: INT64\n",
      "logical_type: None\n",
      "converted_type (legacy): NONE\n",
      "compression: SNAPPY (space_saved: -5%)\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "%ls ./base_statistics.parquet/\n",
    "! parquet-tools inspect ./base_statistics.parquet/part-00000-17fd217a-ca74-4556-bb0b-991cee8924dc-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь произвём операцию чтения сохранённых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:56:27.179866Z",
     "start_time": "2023-05-03T03:56:26.910581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n",
      "|min_salary|mean_salary|max_salary|\n",
      "+----------+-----------+----------+\n",
      "|      2000|     3400.0|      4600|\n",
      "+----------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DataFrame[min_salary: int, mean_salary: double, max_salary: int], None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_df = (\n",
    "    spark.read\n",
    "        .format('csv')\n",
    "        .option(\"inferSchema\", True)\n",
    "        .option(\"header\", True)\n",
    "        .option(\"sep\", ',')\n",
    "        .load('./base_statistics.csv')\n",
    ")\n",
    "loaded_df, loaded_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из вывода все данные были сохранены корректно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Завершение работы`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После завершения работы не забываем осовободить ресурсы и остановить текущую сессию spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kxEu0kipR2jE",
    "mEuclFqwR5e0",
    "7SOyISvrSIun",
    "HCsO86XSW9kW",
    "lO38c_qmcYCT",
    "ybPKx2_0eJAG",
    "TtBLc8D6hJGb",
    "dfn4tqdooajR"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
