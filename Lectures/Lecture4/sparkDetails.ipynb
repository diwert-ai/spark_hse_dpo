{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Промышленное машинное обучение на Spark`\n",
    "## `Занятие 04: Детали SQL и Spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О чём можно узнать из этого ноутбука:\n",
    "\n",
    "* RDD API\n",
    "* Pivot/Unpivot\n",
    "* Window function\n",
    "* UDF\n",
    "* Формат Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для эффективной работы UDF необходимо установить библиотеку, реализующую передачу данных между Spark и Python в Arrow формате:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:02.362387Z",
     "start_time": "2023-02-05T17:46:59.647723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/evgeniy/.local/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: pyarrow in /home/evgeniy/.local/lib/python3.10/site-packages (13.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/evgeniy/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/evgeniy/.local/lib/python3.10/site-packages (from pyarrow) (1.26.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pyspark pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:13.832913Z",
     "start_time": "2023-02-05T17:47:04.417093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 22:40:16 WARN Utils: Your hostname, vm-01 resolves to a loopback address: 127.0.1.1; using 10.128.0.16 instead (on interface eth0)\n",
      "23/10/27 22:40:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/27 22:40:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/27 22:40:17 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "        .set('spark.ui.port', '4050')\n",
    "        .setMaster('local[*]')\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfn4tqdooajR"
   },
   "source": [
    "### `Spark RDD API`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо рассмотренных ранне API (DataFrame API & SQL AP) работы с данными в спарк, существует ещё один интерфейс - RDD API. В нём производится работа с RDD напрямую."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напоминание\n",
    "\n",
    "<div style='font-size:1.2em; background-color:#90e0ef; border-radius: 40px; padding:2em; margin: 0 10px 20px'> \n",
    "    <b>RDD </b>(resilent distrubuted dataset) - это фундаментальная структура данных Spark, которая представляет собой неизменяемый набор данных, который вычисляются и располагается на разных узлах кластера.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rdd_repetition](images/rdd_repetition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6XuSx2Eo16N"
   },
   "source": [
    "* [Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "* [Документация](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы создать RDD необходимо к некоторой коллекции объектов применимеить операциюю parallelize. В результате работы spark разобъёт данные на куски (партиции) и отправит её части на разные worker ноды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:14.929849Z",
     "start_time": "2023-02-05T17:47:14.670825Z"
    },
    "id": "oFdQjWbfouNJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 10),\n",
    "    (2, 41),\n",
    "    (0, 12),\n",
    "    (2, 64),\n",
    "    (2, 22),\n",
    "    (1, 11),\n",
    "    (0, 94),\n",
    "]\n",
    "dist_data = sc.parallelize(data)\n",
    "dist_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратной операцией к операции parallelize является метод collect, который наоборот создаёт коллекцию из данных, хранящихся на различных worker нодах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:16.109485Z",
     "start_time": "2023-02-05T17:47:15.432580Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWDkHLUbpRrA",
    "outputId": "186df13f-6485-4e3b-8b6a-c53fd23673a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10), (2, 41), (0, 12), (2, 64), (2, 22), (1, 11), (0, 94)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parallelize-collect](images/parallelize-collect.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:17.631983Z",
     "start_time": "2023-02-05T17:47:16.794842Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8lXGP8NpTOM",
    "outputId": "f2e5c344-9986-4955-c5bf-0c64b3f0bc95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(11, 1), (43, 2), (12, 0), (66, 2), (24, 2), (12, 1), (94, 0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# К элементам RDD также как и к SparkDataFrame применять различные операции\n",
    "dist_data.map(lambda x: (x[0] + x[1], x[0])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно перевести RDD в Spark.DataFrame и обратно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:23.064113Z",
     "start_time": "2023-02-05T17:47:18.261513Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NivHzNFppYVs",
    "outputId": "c79c44b6-c649-436f-8931-9e7812eda325"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[key: bigint, value: bigint],\n",
       " MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD -> Spark DataFrame \n",
    "dist_df = dist_data.toDF(['key', 'value'])\n",
    "\n",
    "# Spark DataFrame -> RDD \n",
    "dist_rdd = dist_df.rdd\n",
    "dist_df, dist_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим операцию групировки по ключу с применением оперции агрегирующей оперцией суммы к значениям, которые имеют один и тот же ключ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:24.589599Z",
     "start_time": "2023-02-05T17:47:23.879654Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LeiWR9VptZU",
    "outputId": "1e9f535f-7097-4cfd-8c51-fd445a849039"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 127), (0, 106), (1, 21)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data.groupByKey().mapValues(sum).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD API поддердивает прямую загрузку из текстового файла. При этом каждая строка будет интерпретироваться, как отдельный элемент RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:25.224632Z",
     "start_time": "2023-02-05T17:47:24.592327Z"
    },
    "id": "oNixwVHXrkfy"
   },
   "outputs": [],
   "source": [
    "! echo \"Hello, sample RDD\" > text.txt\n",
    "! echo \"This RDD contains three lines\" >> text.txt\n",
    "! echo \"This is the last line\" >> text.txt\n",
    "! echo \"\" >> text.txt\n",
    "! echo \"Just kidding, it contains five lines\" >> text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:25.903614Z",
     "start_time": "2023-02-05T17:47:25.648415Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXaOLQ7ksEnO",
    "outputId": "4f5d3461-2625-406a-e550-42198a9c029f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(text.txt MapPartitionsRDD[17] at textFile at NativeMethodAccessorImpl.java:0,\n",
       " ['Hello, sample RDD',\n",
       "  'This RDD contains three lines',\n",
       "  'This is the last line',\n",
       "  '',\n",
       "  'Just kidding, it contains five lines'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = sc.textFile('text.txt')\n",
    "text_data, text_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь к данному RDD можно применять стандартные Spark операции "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:26.897676Z",
     "start_time": "2023-02-05T17:47:26.451644Z"
    },
    "id": "PoNCwEfFsHvF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[25] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[21] at mapPartitions at PythonRDD.scala:160 []\n",
      " |  ShuffledRDD[20] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[19] at distinct at /tmp/ipykernel_2175562/1777586505.py:5 []\n",
      "    |  PythonRDD[18] at distinct at /tmp/ipykernel_2175562/1777586505.py:5 []\n",
      "    |  text.txt MapPartitionsRDD[17] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  text.txt HadoopRDD[16] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "distinct_words = (\n",
    "    text_data\n",
    "        .filter(lambda x: len(x)) # отбираем только не пустые строки\n",
    "        .flatMap(lambda x: x.split(' ')) # разбиваем все строки на слова и переводим список\n",
    "        .distinct() # берём только уникальные слова\n",
    ")\n",
    "# Будьте внимательны, если такой файл существует,\n",
    "# то spark будет выдавать ошибку\n",
    "distinct_words.saveAsTextFile('words.txt')\n",
    "\n",
    "# можно вывести отладочную информацию по данному RDD\n",
    "print(distinct_words.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снизу вверх показаны все низкоуровневые операции (lineage), которые были применены к данному RDD c самого начала его создания "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы переиспользовать посчитанные значения в рамках текущей сессии стоит использовать метод `.cache`, который сохраняет результат вычислений вершины графа вычислений в оперативной памяти. Это нужно для того, чтобы оперции, работающие поверх данной получали результат операций из оперативной памяти, а не считывались с диска.\n",
    "\n",
    "Метод `.persist` позволяет сохранять промежуточные вычисления в рамках текущей сессии с более тонкой настройкой места хранения (жёсткий диск, оперативная память, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:27.926226Z",
     "start_time": "2023-02-05T17:47:27.915019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[25] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
      " |  MapPartitionsRDD[21] at mapPartitions at PythonRDD.scala:160 [Memory Serialized 1x Replicated]\n",
      " |  ShuffledRDD[20] at partitionBy at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n",
      " +-(2) PairwiseRDD[19] at distinct at /tmp/ipykernel_2175562/1777586505.py:5 [Memory Serialized 1x Replicated]\n",
      "    |  PythonRDD[18] at distinct at /tmp/ipykernel_2175562/1777586505.py:5 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt MapPartitionsRDD[17] at textFile at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt HadoopRDD[16] at textFile at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "distinct_words_cached = distinct_words.cache()\n",
    "print(distinct_words_cached.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить после операции cache появились дополнительные вершины в графе вычислений, в которых указаны место расположения данных и количество их реплик: Memory Serialized 1x Replicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:29.725557Z",
     "start_time": "2023-02-05T17:47:29.600868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[25] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n",
      " |       CachedPartitions: 2; MemorySize: 298.0 B; DiskSize: 0.0 B\n",
      " |  MapPartitionsRDD[21] at mapPartitions at PythonRDD.scala:160 [Memory Serialized 1x Replicated]\n",
      " |  ShuffledRDD[20] at partitionBy at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n",
      " +-(2) PairwiseRDD[19] at distinct at /tmp/ipykernel_2175562/1777586505.py:5 [Memory Serialized 1x Replicated]\n",
      "    |  PythonRDD[18] at distinct at /tmp/ipykernel_2175562/1777586505.py:5 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt MapPartitionsRDD[17] at textFile at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n",
      "    |  text.txt HadoopRDD[16] at textFile at NativeMethodAccessorImpl.java:0 [Memory Serialized 1x Replicated]\n"
     ]
    }
   ],
   "source": [
    "distinct_words_cached.collect()\n",
    "print(distinct_words_cached.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сохранения данных между сессиями можно использовать `.checkpoint`. Особенность этого метода — изменение графа вычислений.\n",
    "Цепочка вычислений для сохраняемого RDD будет удалена.\n",
    "\n",
    "Сокращение цепочки вычислений полезно в случае больших графов, например, в итеративных алгоритмах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:32.943109Z",
     "start_time": "2023-02-05T17:47:32.903993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[30] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[29] at mapPartitions at PythonRDD.scala:160 []\n",
      " |  ShuffledRDD[28] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[27] at distinct at /tmp/ipykernel_2175562/2415635507.py:5 []\n",
      "    |  PythonRDD[26] at distinct at /tmp/ipykernel_2175562/2415635507.py:5 []\n",
      "    |  text.txt MapPartitionsRDD[17] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  text.txt HadoopRDD[16] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "distinct_first_words = (\n",
    "    text_data\n",
    "        .filter(lambda x: len(x))\n",
    "        .flatMap(lambda x: x.split(' ')[0])\n",
    "        .distinct()\n",
    ")\n",
    "\n",
    "sc.setCheckpointDir('./checkpoints')\n",
    "\n",
    "distinct_first_words.checkpoint()\n",
    "print(distinct_first_words.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:35.168178Z",
     "start_time": "2023-02-05T17:47:34.634503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[30] at RDD at PythonRDD.scala:53 []\n",
      " |  ReliableCheckpointRDD[31] at collect at /tmp/ipykernel_2175562/656455338.py:1 []\n"
     ]
    }
   ],
   "source": [
    "distinct_first_words.collect()\n",
    "print(distinct_first_words.toDebugString().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно увидеть предыдущий граф вычилений был полность удалён, и теперь вычисления начинаются с загрузки контрольной точки: ReliableCheckpointRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Pivot/Unpivot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем модельные данные и распакуем их в директорию ml-1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:41.797212Z",
     "start_time": "2023-02-05T17:47:38.864468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-27 22:40:37--  https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5917549 (5.6M) [application/zip]\n",
      "Saving to: ‘ml-1m.zip.2’\n",
      "\n",
      "ml-1m.zip.2         100%[===================>]   5.64M  3.91MB/s    in 1.4s    \n",
      "\n",
      "2023-10-27 22:40:40 (3.91 MB/s) - ‘ml-1m.zip.2’ saved [5917549/5917549]\n",
      "\n",
      "Archive:  ml-1m.zip\n",
      "   creating: ml-1m/\n",
      "  inflating: ml-1m/movies.dat        \n",
      "  inflating: ml-1m/ratings.dat       \n",
      "  inflating: ml-1m/README            \n",
      "  inflating: ml-1m/users.dat         \n"
     ]
    }
   ],
   "source": [
    "! wget https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "! unzip -o ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:42.847648Z",
     "start_time": "2023-02-05T17:47:42.637978Z"
    }
   },
   "outputs": [],
   "source": [
    "# указываем схему данных для первой таблицы\n",
    "schema = (\n",
    "    T.StructType()\n",
    "        .add('movie_id', T.IntegerType()) # колонка movie_id будет иметь целочиселнный тип\n",
    "        .add('movie', T.StringType()) # колонка movie будет иметь строковый\n",
    "        .add('categories', T.StringType()) # и колонка categories также будет строкой\n",
    ")\n",
    "# загружаем в spark данные из первой таблицы\n",
    "movies_df = (\n",
    "    spark.read\n",
    "      .format('csv') # указываем формат считываем данных\n",
    "      .option(\"header\", False) # считываем данные без заголовка\n",
    "      .option(\"sep\", '::') # в качестве разделителя столбцов указываем ::\n",
    "      .schema(schema) # в качестве схемы указываем описанную выше схему\n",
    "      .load('./ml-1m/movies.dat') # указываем источник данных\n",
    ")\n",
    "\n",
    "# тоже самое только для второй таблицы\n",
    "schema = (\n",
    "    T.StructType()\n",
    "        .add('user_id', T.IntegerType())\n",
    "        .add('movie_id', T.IntegerType())\n",
    "        .add('rating', T.FloatType()) # данный тип - вещественные числа\n",
    "        .add('timestamp', T.StringType())\n",
    ")\n",
    "ratings_df = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"header\", False)\n",
    "      .option(\"sep\", '::')\n",
    "      .schema(schema)\n",
    "      .load('./ml-1m/ratings.dat')\n",
    ")\n",
    "\n",
    "# тоже самое только для третьей таблицы\n",
    "schema = (\n",
    "    T.StructType()\n",
    "        .add('user_id', T.IntegerType())\n",
    "        .add('gender', T.StringType())\n",
    "        .add('age', T.IntegerType())\n",
    "        .add('occupation', T.IntegerType())\n",
    "        .add('zip-code', T.StringType())\n",
    ")\n",
    "users_df = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"header\", False)\n",
    "      .option(\"sep\", '::')\n",
    "      .schema(schema)\n",
    "      .load('./ml-1m/users.dat')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во многих задачах зачастую встречается необходимость сформировать колонки на основе данных, хранящихся в другом столбце. То есть требуется перевести таблицу в wide формат.\n",
    "\n",
    "Например, данная операция нужна была во второй лекции, когда для значения класса нужно было получить one-hot представление"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Визуализация того, как это устроено в pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pandas-pivot](images/pandas-dataframe-pivot-1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark также поддерживает данную операцию, но делается это вызовом не одного метода, а нескольких последовательных:\n",
    "\n",
    "groupBy -> pivot -> agg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Допустим, что имеется следующая таблица\n",
    "\n",
    "+-------+--------+------+\n",
    "|user_id|movie_id|rating|\n",
    "+-------+--------+------+\n",
    "|      1|    1193|   5.0|\n",
    "|      1|    1193|   4.0|\n",
    "|      1|     661|   3.0|\n",
    "|      2|     661|   3.0|\n",
    "|      3|    1193|   4.0|\n",
    "|      3|    2355|   5.0|\n",
    "+-------+--------+------+\n",
    "\n",
    "Для того чтобы перевести её в wide формат вызывае описанные выше функции\n",
    "\n",
    "          index        columns       values   \n",
    "            |             |            |\n",
    "groupBy(user_id).pivot(movie_id).agg(mean(rating))\n",
    "groupBy(user_id).pivot(movie_id).agg(first(rating))\n",
    "\n",
    "После groupBy:\n",
    "\n",
    "1 -> (1193, 5.0), (1193, 4.0), (661, 3.0)\n",
    "2 -> (661, 3.0)\n",
    "3 -> (1193, 4.0), (2355, 5.0)\n",
    "\n",
    "После операции pivot\n",
    "\n",
    "        1193      |    661   |    2355\n",
    "1 ->  [5.0, 4.0]  |   [3.0]  |    \n",
    "2 ->              |   [3.0]  |     \n",
    "3 ->     [4.0]    |          |    [5.0]\n",
    "\n",
    "После выполнения агрегации agg\n",
    "\n",
    "   1193 661 2355\n",
    "1  4.5  3.0 None\n",
    "2  None 3.0 None\n",
    "3  4.0 None 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:44.767745Z",
     "start_time": "2023-02-05T17:47:44.338940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|      1|    1193|   5.0|978300760|\n",
      "|      1|     661|   3.0|978302109|\n",
      "|      1|     914|   3.0|978301968|\n",
      "|      1|    3408|   4.0|978300275|\n",
      "|      1|    2355|   5.0|978824291|\n",
      "|      1|    1197|   3.0|978302268|\n",
      "|      1|    1287|   5.0|978302039|\n",
      "|      1|    2804|   5.0|978300719|\n",
      "|      1|     594|   4.0|978302268|\n",
      "|      1|     919|   4.0|978301368|\n",
      "|      1|     595|   5.0|978824268|\n",
      "|      1|     938|   4.0|978301752|\n",
      "|      1|    2398|   4.0|978302281|\n",
      "|      1|    2918|   4.0|978302124|\n",
      "|      1|    1035|   5.0|978301753|\n",
      "|      1|    2791|   4.0|978302188|\n",
      "|      1|    2687|   3.0|978824268|\n",
      "|      1|    2018|   4.0|978301777|\n",
      "|      1|    3105|   5.0|978301713|\n",
      "|      1|    2797|   4.0|978302039|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:55.452913Z",
     "start_time": "2023-02-05T17:47:45.148611Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 22:40:49 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/10/27 22:40:59 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB                                                                                    (0 + 1) / 1]\n",
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3943</th>\n",
       "      <th>3944</th>\n",
       "      <th>3945</th>\n",
       "      <th>3946</th>\n",
       "      <th>3947</th>\n",
       "      <th>3948</th>\n",
       "      <th>3949</th>\n",
       "      <th>3950</th>\n",
       "      <th>3951</th>\n",
       "      <th>3952</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3707 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    1   2   3   4   5   6   7   8   9  ...  3943  3944  3945  3946  \\\n",
       "0        1  5.0 NaN NaN NaN NaN NaN NaN NaN NaN  ...   NaN   NaN   NaN   NaN   \n",
       "1        3  NaN NaN NaN NaN NaN NaN NaN NaN NaN  ...   NaN   NaN   NaN   NaN   \n",
       "2        2  NaN NaN NaN NaN NaN NaN NaN NaN NaN  ...   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   3947  3948  3949  3950  3951  3952  \n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[3 rows x 3707 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_df = (\n",
    "    ratings_df\n",
    "        .groupBy(ratings_df.user_id)\n",
    "        .pivot('movie_id')\n",
    "        .agg(F.first(ratings_df.rating))\n",
    ")\n",
    "pivot_df.where(pivot_df.user_id < 4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:56.214419Z",
     "start_time": "2023-02-05T17:47:55.455273Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:===============================================================================>                                                                               (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------------+\n",
      "|movie_id|rates|        avg_rating|\n",
      "+--------+-----+------------------+\n",
      "|    2858| 3428|4.3173862310385065|\n",
      "|     260| 2991| 4.453694416583082|\n",
      "|    1196| 2990| 4.292976588628763|\n",
      "|    1210| 2883| 4.022892819979188|\n",
      "|     480| 2672|3.7638473053892216|\n",
      "|    2028| 2653| 4.337353938937053|\n",
      "|     589| 2649| 4.058512646281616|\n",
      "|    2571| 2590| 4.315830115830116|\n",
      "|    1270| 2583|3.9903213317847466|\n",
      "|     593| 2578|4.3518231186966645|\n",
      "|    1580| 2538| 3.739952718676123|\n",
      "|    1198| 2514| 4.477724741447892|\n",
      "|     608| 2513| 4.254675686430561|\n",
      "|    2762| 2459| 4.406262708418057|\n",
      "|     110| 2443| 4.234957020057307|\n",
      "|    2396| 2369| 4.127479949345715|\n",
      "|    1197| 2318|4.3037100949094045|\n",
      "|     527| 2304| 4.510416666666667|\n",
      "|    1617| 2288| 4.219405594405594|\n",
      "|    1265| 2278| 3.953028972783143|\n",
      "+--------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "top_movies_df = ratings_df.groupBy(\n",
    "    ratings_df.movie_id # групируем по movie_id\n",
    ").agg(\n",
    "    F.count(ratings_df.rating).alias('rates'), # создаём колонку rates c количество оценок\n",
    "    F.mean(ratings_df.rating).alias('avg_rating') # создаём колонку avg_rating с средней оценкой\n",
    ").sort(\n",
    "    'rates', ascending=False # упорядочиваем фильмы по количеству оценок в убывающем порядке\n",
    ").limit(100) # берём только 100 самых оцениваемых фильмов\n",
    "\n",
    "top_movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:56.755558Z",
     "start_time": "2023-02-05T17:47:56.217120Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "top_movies = top_movies_df.rdd.map(lambda x: x.movie_id).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:57.536251Z",
     "start_time": "2023-02-05T17:47:56.759503Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>2858</th>\n",
       "      <th>260</th>\n",
       "      <th>1196</th>\n",
       "      <th>1210</th>\n",
       "      <th>480</th>\n",
       "      <th>2028</th>\n",
       "      <th>589</th>\n",
       "      <th>2571</th>\n",
       "      <th>1270</th>\n",
       "      <th>...</th>\n",
       "      <th>750</th>\n",
       "      <th>2699</th>\n",
       "      <th>39</th>\n",
       "      <th>21</th>\n",
       "      <th>1393</th>\n",
       "      <th>2804</th>\n",
       "      <th>588</th>\n",
       "      <th>2406</th>\n",
       "      <th>1220</th>\n",
       "      <th>733</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  2858  260  1196  1210  480  2028  589  2571  1270  ...  750  2699  \\\n",
       "0        1   NaN  4.0   NaN   NaN  NaN   5.0  NaN   NaN   5.0  ...  NaN   NaN   \n",
       "1        2   4.0  NaN   5.0   4.0  5.0   4.0  4.0   4.0   NaN  ...  NaN   NaN   \n",
       "\n",
       "   39   21  1393  2804  588  2406  1220  733  \n",
       "0 NaN  NaN   NaN   5.0  4.0   NaN   NaN  NaN  \n",
       "1 NaN  1.0   NaN   NaN  NaN   NaN   NaN  NaN  \n",
       "\n",
       "[2 rows x 101 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_top_df = (\n",
    "    ratings_df\n",
    "        .groupBy(ratings_df.user_id)\n",
    "        .pivot('movie_id', top_movies)\n",
    "        .agg(F.first(ratings_df.rating))\n",
    ")\n",
    "pivot_top_df.where(pivot_df.user_id < 3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве второго аргумента в методе pivot можно указать подмножество допустимых значений на основе, которых сформируются новые столбцы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:58.712056Z",
     "start_time": "2023-02-05T17:47:57.539033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>2858</th>\n",
       "      <th>260</th>\n",
       "      <th>1196</th>\n",
       "      <th>1210</th>\n",
       "      <th>480</th>\n",
       "      <th>2028</th>\n",
       "      <th>589</th>\n",
       "      <th>2571</th>\n",
       "      <th>1270</th>\n",
       "      <th>...</th>\n",
       "      <th>750</th>\n",
       "      <th>2699</th>\n",
       "      <th>39</th>\n",
       "      <th>21</th>\n",
       "      <th>1393</th>\n",
       "      <th>2804</th>\n",
       "      <th>588</th>\n",
       "      <th>2406</th>\n",
       "      <th>1220</th>\n",
       "      <th>733</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  2858  260  1196  1210  480  2028  589  2571  1270  ...  750  2699  \\\n",
       "0        1   NaN  4.0   NaN   NaN  NaN   5.0  NaN   NaN   5.0  ...  NaN   NaN   \n",
       "1        3   4.0  5.0   4.0   4.0  4.0   NaN  NaN   NaN   3.0  ...  NaN   NaN   \n",
       "2        2   4.0  NaN   5.0   4.0  5.0   4.0  4.0   4.0   NaN  ...  NaN   NaN   \n",
       "\n",
       "   39   21  1393  2804  588  2406  1220  733  \n",
       "0 NaN  NaN   NaN   5.0  4.0   NaN   NaN  NaN  \n",
       "1 NaN  NaN   NaN   NaN  NaN   NaN   NaN  5.0  \n",
       "2 NaN  1.0   NaN   NaN  NaN   NaN   NaN  NaN  \n",
       "\n",
       "[3 rows x 101 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# формируем таблицу с столбцы только по top-100 самых оцениваемых фильмов\n",
    "pivot_top_df = (\n",
    "    ratings_df\n",
    "        .groupBy(ratings_df.user_id)\n",
    "        .pivot('movie_id', top_movies) # вторым параметром ограничиваем дотупстимые столбцы\n",
    "        .agg(F.first(ratings_df.rating))\n",
    ")\n",
    "pivot_top_df.where(pivot_top_df.user_id < 4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить кол-во пустых столбцов стало гораздо меньше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем данные из предыдущей лекции, с которыми будем в дальнейшем работать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "folder_url = 'https://disk.yandex.lt/d/JnDy1h48pJI7IA'\n",
    "file_url = '/m5-forecating-accuracy.zip'\n",
    "# запрос ссылки на скачивание\n",
    "response = requests.get('https://cloud-api.yandex.net/v1/disk/public/resources/download',\n",
    "                 params={'public_key': folder_url, 'path': file_url}) \n",
    "# 'парсинг' ссылки на скачивание\n",
    "data_link = response.json()['href'] \t\n",
    "\n",
    "filename = 'm5-forecating-accuracy.zip'\n",
    "path = \"./m5-forecasting-accuracy\"\n",
    "\n",
    "# запускаем скачивание вызовом команды wget из python\n",
    "subprocess.run(\n",
    "    ['wget', '-O', filename, data_link], # команда для исполнения\n",
    "    stdout=subprocess.DEVNULL, # убираем печать отладочной информации\n",
    "    stderr=subprocess.STDOUT\n",
    ")\n",
    "\n",
    "#распакуем данные по пути path\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:03.087299Z",
     "start_time": "2023-02-05T17:47:58.714144Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1904</th>\n",
       "      <th>d_1905</th>\n",
       "      <th>d_1906</th>\n",
       "      <th>d_1907</th>\n",
       "      <th>d_1908</th>\n",
       "      <th>d_1909</th>\n",
       "      <th>d_1910</th>\n",
       "      <th>d_1911</th>\n",
       "      <th>d_1912</th>\n",
       "      <th>d_1913</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_006_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_006</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_007_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_007</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_010</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "5  HOBBIES_1_006_CA_1_validation  HOBBIES_1_006  HOBBIES_1  HOBBIES     CA_1   \n",
       "6  HOBBIES_1_007_CA_1_validation  HOBBIES_1_007  HOBBIES_1  HOBBIES     CA_1   \n",
       "7  HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1   \n",
       "8  HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES     CA_1   \n",
       "9  HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1904  d_1905  d_1906  d_1907  d_1908  \\\n",
       "0       CA    0    0    0    0  ...       1       3       0       1       1   \n",
       "1       CA    0    0    0    0  ...       0       0       0       0       0   \n",
       "2       CA    0    0    0    0  ...       2       1       2       1       1   \n",
       "3       CA    0    0    0    0  ...       1       0       5       4       1   \n",
       "4       CA    0    0    0    0  ...       2       1       1       0       1   \n",
       "5       CA    0    0    0    0  ...       0       1       0       1       0   \n",
       "6       CA    0    0    0    0  ...       0       0       0       1       0   \n",
       "7       CA   12   15    0    0  ...       0       0       1      37       3   \n",
       "8       CA    2    0    7    3  ...       0       0       1       1       6   \n",
       "9       CA    0    0    1    0  ...       1       0       0       0       0   \n",
       "\n",
       "   d_1909  d_1910  d_1911  d_1912  d_1913  \n",
       "0       1       3       0       1       1  \n",
       "1       1       0       0       0       0  \n",
       "2       1       0       1       1       1  \n",
       "3       0       1       3       7       2  \n",
       "4       1       2       2       2       4  \n",
       "5       0       0       2       0       0  \n",
       "6       1       0       0       1       1  \n",
       "7       4       6       3       2       1  \n",
       "8       0       0       0       0       0  \n",
       "9       0       0       2       0       2  \n",
       "\n",
       "[10 rows x 1919 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# считаем данные\n",
    "df_validation = (\n",
    "    spark.read.format('csv')\n",
    "      .option(\"inferSchema\", True) # указываем spark самому определить тип данных\n",
    "      .option(\"header\", True)\n",
    "      .option(\"sep\", ',')\n",
    "      .load(f\"{path}/sales_train_validation.csv\")\n",
    ")\n",
    "df_validation.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для операции pivot существует противоположная операция \"вытягивания\" таблицы в длину на основе значений из столбцов.\n",
    "\n",
    "Для этого применяется оперция stack:\n",
    "```SQL\n",
    "stack(\n",
    "    N, new_value_name_1, old_colname_1, ..., new_value_name_N, old_colname_N\n",
    ") as (new_colname, value_colname)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:03.372957Z",
     "start_time": "2023-02-05T17:48:03.089371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------+-----+\n",
      "|id                           |d      |sales|\n",
      "+-----------------------------+-------+-----+\n",
      "|HOBBIES_1_001_CA_1_validation|d_1_new|0    |\n",
      "|HOBBIES_1_001_CA_1_validation|d_2    |0    |\n",
      "|HOBBIES_1_002_CA_1_validation|d_1_new|0    |\n",
      "|HOBBIES_1_002_CA_1_validation|d_2    |0    |\n",
      "|HOBBIES_1_003_CA_1_validation|d_1_new|0    |\n",
      "|HOBBIES_1_003_CA_1_validation|d_2    |0    |\n",
      "|HOBBIES_1_004_CA_1_validation|d_1_new|0    |\n",
      "|HOBBIES_1_004_CA_1_validation|d_2    |0    |\n",
      "|HOBBIES_1_005_CA_1_validation|d_1_new|0    |\n",
      "|HOBBIES_1_005_CA_1_validation|d_2    |0    |\n",
      "|HOBBIES_1_006_CA_1_validation|d_1_new|0    |\n",
      "|HOBBIES_1_006_CA_1_validation|d_2    |0    |\n",
      "|HOBBIES_1_007_CA_1_validation|d_1_new|0    |\n",
      "|HOBBIES_1_007_CA_1_validation|d_2    |0    |\n",
      "|HOBBIES_1_008_CA_1_validation|d_1_new|12   |\n",
      "|HOBBIES_1_008_CA_1_validation|d_2    |15   |\n",
      "|HOBBIES_1_009_CA_1_validation|d_1_new|2    |\n",
      "|HOBBIES_1_009_CA_1_validation|d_2    |0    |\n",
      "|HOBBIES_1_010_CA_1_validation|d_1_new|0    |\n",
      "|HOBBIES_1_010_CA_1_validation|d_2    |0    |\n",
      "+-----------------------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivot_expression = \"stack(2, 'd_1_new', d_1, 'd_2', d_2) as (d, sales)\"\n",
    "unpivot_df = (\n",
    "    df_validation\n",
    "        .select('id', F.expr(unpivot_expression)) \n",
    ")\n",
    "\n",
    "unpivot_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь произвём обратную оперцию \"расширения\" (pivot) для новой таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:05.528901Z",
     "start_time": "2023-02-05T17:48:03.376408Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                                                                                                                              (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------+---+\n",
      "|id                             |d_1_new|d_2|\n",
      "+-------------------------------+-------+---+\n",
      "|HOBBIES_1_304_CA_4_validation  |0      |0  |\n",
      "|FOODS_3_257_CA_3_validation    |1      |0  |\n",
      "|HOUSEHOLD_1_157_CA_2_validation|0      |0  |\n",
      "|FOODS_2_387_CA_1_validation    |0      |0  |\n",
      "|FOODS_2_225_CA_3_validation    |4      |6  |\n",
      "|FOODS_2_375_CA_2_validation    |0      |0  |\n",
      "|FOODS_2_354_CA_4_validation    |0      |0  |\n",
      "|FOODS_3_297_TX_1_validation    |0      |0  |\n",
      "|HOUSEHOLD_1_319_CA_3_validation|1      |1  |\n",
      "|HOUSEHOLD_2_090_CA_4_validation|0      |0  |\n",
      "|FOODS_3_568_CA_4_validation    |0      |0  |\n",
      "|HOBBIES_1_364_TX_2_validation  |3      |0  |\n",
      "|FOODS_1_101_CA_2_validation    |0      |0  |\n",
      "|FOODS_3_558_TX_1_validation    |1      |2  |\n",
      "|FOODS_2_011_CA_1_validation    |1      |1  |\n",
      "|FOODS_3_035_CA_3_validation    |3      |2  |\n",
      "|FOODS_2_174_CA_2_validation    |0      |0  |\n",
      "|HOBBIES_1_396_CA_4_validation  |0      |0  |\n",
      "|HOBBIES_2_044_CA_2_validation  |0      |0  |\n",
      "|HOBBIES_1_320_CA_3_validation  |7      |2  |\n",
      "+-------------------------------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    unpivot_df\n",
    "        .groupBy(unpivot_df.id)\n",
    "        .pivot('d')\n",
    "        .agg(F.sum(unpivot_df.sales))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что в ходе операций unpivot->pivot данные остались такими же какими они и были изначально, взяв последнюю строку из таблицы выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:06.073317Z",
     "start_time": "2023-02-05T17:48:05.531766Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---+---+\n",
      "|id                           |d_1|d_2|\n",
      "+-----------------------------+---+---+\n",
      "|HOBBIES_1_320_CA_3_validation|7  |2  |\n",
      "+-----------------------------+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_validation\n",
    "        .where(df_validation.id == 'HOBBIES_1_320_CA_3_validation')\n",
    "        .select('id', 'd_1', 'd_2')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Window function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В SQL запросах часто встречаются оконные функции, которые необходимы при различных для аналитических задач с данными: \n",
    "\n",
    " - нарастающие итоги\n",
    " - скользящие средние\n",
    " - ранжирование\n",
    "\n",
    "\n",
    "**Оконная функция в SQL** - функция, которая работает с выделенным набором строк (окном, партицией) и выполняет вычисление для этого набора строк в отдельном столбце. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Сравнение оконной функции с групировкой`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![window_func](images/window_func.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В pyspark также есть возможность работы с оконными функциями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:16.350717Z",
     "start_time": "2023-02-05T17:48:16.347907Z"
    }
   },
   "outputs": [],
   "source": [
    "# импортируем модуль, в котором располагаются оконные функции\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:17.196801Z",
     "start_time": "2023-02-05T17:48:16.872989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------+---------+------+\n",
      "|emp_id |dept_id  |salary|\n",
      "+-------+---------+------+\n",
      "|James  |Sales    |3000  |\n",
      "|Michael|Sales    |4600  |\n",
      "|Robert |Sales    |4100  |\n",
      "|Maria  |Finance  |3000  |\n",
      "|Scott  |Finance  |3300  |\n",
      "|Jen    |Finance  |3900  |\n",
      "|Jeff   |Marketing|3000  |\n",
      "|Kumar  |Marketing|2000  |\n",
      "|Saif   |Sales    |4100  |\n",
      "+-------+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Michael', 'Sales', 4600),\n",
    "    ('Robert', 'Sales', 4100),\n",
    "    ('Maria', 'Finance', 3000),\n",
    "    ('Scott', 'Finance', 3300),\n",
    "    ('Jen', 'Finance', 3900), \n",
    "    ('Jeff', 'Marketing', 3000),\n",
    "    ('Kumar', 'Marketing', 2000),\n",
    "    ('Saif', 'Sales', 4100)\n",
    "]\n",
    " \n",
    "columns = [\"emp_id\", \"dept_id\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.printSchema(), df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cоздадим новую таблицу, в которой будет с агрегирована информация о средней зарплате в пределах департамента и объеденим её с таблицей с информацией о сотрудниках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:18.373245Z",
     "start_time": "2023-02-05T17:48:17.740127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+----------+\n",
      "|  dept_id| emp_id|salary|avg_salary|\n",
      "+---------+-------+------+----------+\n",
      "|    Sales|  James|  3000|    3950.0|\n",
      "|    Sales|Michael|  4600|    3950.0|\n",
      "|    Sales| Robert|  4100|    3950.0|\n",
      "|  Finance|  Maria|  3000|    3400.0|\n",
      "|    Sales|   Saif|  4100|    3950.0|\n",
      "|  Finance|  Scott|  3300|    3400.0|\n",
      "|  Finance|    Jen|  3900|    3400.0|\n",
      "|Marketing|   Jeff|  3000|    2500.0|\n",
      "|Marketing|  Kumar|  2000|    2500.0|\n",
      "+---------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_salaries_df = df.groupBy(\n",
    "    df.dept_id # по какому полю агрегируем\n",
    ").agg(\n",
    "    F.mean(df.salary # функция агрегации\n",
    ").alias('avg_salary')) # задаём имя новой колнке\n",
    "\n",
    "# объединяем с таблицей сотрудников\n",
    "df.join(avg_salaries_df, on='dept_id', how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:18.796575Z",
     "start_time": "2023-02-05T17:48:18.376876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------------------------+----------+\n",
      "|emp_id |dept_id  |salary|salaries_list           |avg_salary|\n",
      "+-------+---------+------+------------------------+----------+\n",
      "|Maria  |Finance  |3000  |[3000, 3300, 3900]      |3400.0    |\n",
      "|Scott  |Finance  |3300  |[3000, 3300, 3900]      |3400.0    |\n",
      "|Jen    |Finance  |3900  |[3000, 3300, 3900]      |3400.0    |\n",
      "|Jeff   |Marketing|3000  |[3000, 2000]            |2500.0    |\n",
      "|Kumar  |Marketing|2000  |[3000, 2000]            |2500.0    |\n",
      "|James  |Sales    |3000  |[3000, 4600, 4100, 4100]|3950.0    |\n",
      "|Michael|Sales    |4600  |[3000, 4600, 4100, 4100]|3950.0    |\n",
      "|Robert |Sales    |4100  |[3000, 4600, 4100, 4100]|3950.0    |\n",
      "|Saif   |Sales    |4100  |[3000, 4600, 4100, 4100]|3950.0    |\n",
      "+-------+---------+------+------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# создаём окно, в котором указываем по какому полю будет происходит создание окон\n",
    "wspec = Window.partitionBy('dept_id')\n",
    "\n",
    "(\n",
    "    df\n",
    "        .withColumn( # собираем зарплаты сотрудников в пределах своего департамента\n",
    "            'salaries_list',\n",
    "            F.collect_list(df.salary).over(wspec)\n",
    "        ) \n",
    "        .withColumn(\n",
    "            'avg_salary', # считаем среднюю зарплату в пределах департамента\n",
    "            F.mean(df.salary).over(wspec) \n",
    "        )\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо указания полей, по которым будут созданы окна (partitionBy), можно ещё указать по какому полю должны быть упорядочены значения в пределах одного окна (orderBy). При добвалении порядка вычисление значений на основе оконной функции происходит последовательно, а не сразу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:19.222300Z",
     "start_time": "2023-02-05T17:48:18.799787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------------------+----------+------------------+\n",
      "| emp_id|  dept_id|salary|       salaries_list|row_number|        avg_salary|\n",
      "+-------+---------+------+--------------------+----------+------------------+\n",
      "|  Maria|  Finance|  3000|              [3000]|         1|            3000.0|\n",
      "|  Scott|  Finance|  3300|        [3000, 3300]|         2|            3150.0|\n",
      "|    Jen|  Finance|  3900|  [3000, 3300, 3900]|         3|            3400.0|\n",
      "|  Kumar|Marketing|  2000|              [2000]|         1|            2000.0|\n",
      "|   Jeff|Marketing|  3000|        [2000, 3000]|         2|            2500.0|\n",
      "|  James|    Sales|  3000|              [3000]|         1|            3000.0|\n",
      "| Robert|    Sales|  4100|  [3000, 4100, 4100]|         2|3733.3333333333335|\n",
      "|   Saif|    Sales|  4100|  [3000, 4100, 4100]|         3|3733.3333333333335|\n",
      "|Michael|    Sales|  4600|[3000, 4100, 4100...|         4|            3950.0|\n",
      "+-------+---------+------+--------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# создаём окна по полю dep_id и упорядочеваем значения в них на основе поля salary\n",
    "wspec = Window.partitionBy('dept_id').orderBy('salary') \n",
    "(\n",
    "    df\n",
    "        # последовательно собираем список зарплат сотрудников в одном департаменте\n",
    "        .withColumn(\n",
    "            'salaries_list', \n",
    "            F.collect_list(df.salary).over(wspec) \n",
    "        )\n",
    "        # функцией row_number указываем порядок зарплаты сотрудника в пределах своего депратамента\n",
    "        .withColumn( \n",
    "            'row_number', \n",
    "            F.row_number().over(wspec)\n",
    "        )\n",
    "        # последовательно считаем среднеюю зарплату сотрудников в пределах своего департамента\n",
    "        .withColumn(\n",
    "            'avg_salary',\n",
    "            F.mean(df.salary).over(wspec)\n",
    "        )\n",
    ").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вычислении оконных функций можно указать границы окна для текущего элемента через специальный дополнительный метод rowsBetween(start_position, end_position).\n",
    "\n",
    "Например, следуюший вызов:\n",
    " - rowsBetween(0, 1) - будет ограниичивать допустимую границу вычисления для текущего элемента следующей строкой\n",
    " - rowsBetween(-2, 4) - будет ограничивать вычисления двумя предыдущими элементами и четырьмя последующими\n",
    "\n",
    "Для обозначения наиболее частоиспользуемых границ используют ключевые слова:\n",
    " - Window.currentRow - текущая позиция\n",
    " - Window.unboundedFollowing - все последующие значения\n",
    " - Window.unboundedPreceding - все предыдущие значения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rowsWindow](images/rowsWindow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:19.547708Z",
     "start_time": "2023-02-05T17:48:19.224927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------------------+-----------------+\n",
      "| emp_id|  dept_id|salary|       salaries_list|       avg_salary|\n",
      "+-------+---------+------+--------------------+-----------------+\n",
      "|  Maria|  Finance|  3000|  [3000, 3300, 3900]|           3400.0|\n",
      "|  Scott|  Finance|  3300|        [3300, 3900]|           3600.0|\n",
      "|    Jen|  Finance|  3900|              [3900]|           3900.0|\n",
      "|  Kumar|Marketing|  2000|        [2000, 3000]|           2500.0|\n",
      "|   Jeff|Marketing|  3000|              [3000]|           3000.0|\n",
      "|  James|    Sales|  3000|[3000, 4100, 4100...|           3950.0|\n",
      "| Robert|    Sales|  4100|  [4100, 4100, 4600]|4266.666666666667|\n",
      "|   Saif|    Sales|  4100|        [4100, 4600]|           4350.0|\n",
      "|Michael|    Sales|  4600|              [4600]|           4600.0|\n",
      "+-------+---------+------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим окно, в котором в вычислениях затрагиваются только \n",
    "# все последующие элементы окна + текущий\n",
    "wspec = (\n",
    "    Window\n",
    "        .partitionBy('dept_id')\n",
    "        .orderBy('salary')\n",
    "        .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    ")\n",
    "(\n",
    "    df\n",
    "        .withColumn(\n",
    "            'salaries_list', # собираем список зарплат последующих сотрудников + текущий\n",
    "            F.collect_list(df.salary).over(wspec)\n",
    "        )\n",
    "        .withColumn(\n",
    "            'avg_salary', # cчитаем среднюю зарплату для следующих сотрудников + текущий\n",
    "            F.mean(df.salary).over(wspec)\n",
    "        )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо того, что можно указывать границы окна на основе количества последующих и предыдущих элементво в упорядоченном списке, можно указать и границы на основе значения поля по которому происходит упорядочивание. \n",
    "\n",
    "Например, выражение **orderBy(colname).rangeBetween(-400, 400)** для текущего элемента окна с со значением **X** в столбце colname возьмёт только те оконные элементы, у которых значения y в столбце colname удовлетворяет следующему неравенству: \n",
    "\n",
    "X - 400 <= y <= X + 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:19.857532Z",
     "start_time": "2023-02-05T17:48:19.550972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+-------------+----------+\n",
      "| emp_id|  dept_id|salary|salaries_list|avg_salary|\n",
      "+-------+---------+------+-------------+----------+\n",
      "|  Maria|  Finance|  3000| [3000, 3300]|    3150.0|\n",
      "|  Scott|  Finance|  3300| [3000, 3300]|    3150.0|\n",
      "|    Jen|  Finance|  3900|       [3900]|    3900.0|\n",
      "|  Kumar|Marketing|  2000|       [2000]|    2000.0|\n",
      "|   Jeff|Marketing|  3000|       [3000]|    3000.0|\n",
      "|  James|    Sales|  3000|       [3000]|    3000.0|\n",
      "| Robert|    Sales|  4100| [4100, 4100]|    4100.0|\n",
      "|   Saif|    Sales|  4100| [4100, 4100]|    4100.0|\n",
      "|Michael|    Sales|  4600|       [4600]|    4600.0|\n",
      "+-------+---------+------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# считаем значения оконные выражения только для тех сотрудников из одного депратамента,\n",
    "# у которых различие в зарплатах не превосходит 400 у.е.\n",
    "\n",
    "wspec = Window.partitionBy('dept_id').orderBy('salary').rangeBetween(-400, 400)\n",
    "(\n",
    "    df\n",
    "        .withColumn( # собираем значения зарплат сотрудников, попавших в интервал [X-400, X+400]\n",
    "            'salaries_list', \n",
    "            F.collect_list(df.salary).over(wspec)\n",
    "        )\n",
    "        .withColumn( # среднюю зарплату сотрудников, попавших в интервал [X-400, X+400]\n",
    "            'avg_salary', \n",
    "            F.mean(df.salary).over(wspec))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё наиболее частотные оконные функции:\n",
    " - cume_dist - доля элементов, в окне меньших или равных данному\n",
    " - lag(colname, N) - значение столбца сolname, который отстаёт от данного на N элементов\n",
    " - lag(colname, M) - значение столбца сolname, который опережает данный на M элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:20.312702Z",
     "start_time": "2023-02-05T17:48:19.930349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------------------------+------------------+------------------+----+----+\n",
      "|emp_id |dept_id  |salary|salaries_list           |avg_salary        |cume_dist         |lag |lead|\n",
      "+-------+---------+------+------------------------+------------------+------------------+----+----+\n",
      "|Maria  |Finance  |3000  |[3000]                  |3000.0            |0.3333333333333333|NULL|3300|\n",
      "|Scott  |Finance  |3300  |[3000, 3300]            |3150.0            |0.6666666666666666|3000|3900|\n",
      "|Jen    |Finance  |3900  |[3000, 3300, 3900]      |3400.0            |1.0               |3300|NULL|\n",
      "|Kumar  |Marketing|2000  |[2000]                  |2000.0            |0.5               |NULL|3000|\n",
      "|Jeff   |Marketing|3000  |[2000, 3000]            |2500.0            |1.0               |2000|NULL|\n",
      "|James  |Sales    |3000  |[3000]                  |3000.0            |0.25              |NULL|4100|\n",
      "|Robert |Sales    |4100  |[3000, 4100, 4100]      |3733.3333333333335|0.75              |3000|4100|\n",
      "|Saif   |Sales    |4100  |[3000, 4100, 4100]      |3733.3333333333335|0.75              |4100|4600|\n",
      "|Michael|Sales    |4600  |[3000, 4100, 4100, 4600]|3950.0            |1.0               |4100|NULL|\n",
      "+-------+---------+------+------------------------+------------------+------------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wspec = Window.partitionBy('dept_id').orderBy('salary')\n",
    "(\n",
    "    df\n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('avg_salary', F.mean(df.salary).over(wspec))\n",
    "        .withColumn('cume_dist', F.cume_dist().over(wspec))\n",
    "        .withColumn('lag', F.lag(df.salary, 1).over(wspec))\n",
    "        .withColumn('lead', F.lead(df.salary, 1).over(wspec))\n",
    ").show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для работы с порядком элементов в окне:\n",
    "\n",
    " - nth_value - n-ый элментов в текущем окне\n",
    " - ntile - количество элементов в текущем окне меньших чем заданный\n",
    " - dense_rank, rank - функции ранга элмента.Различия в работе в них проявляются, если в столбце встречаются одинковые значения. Хорошо  написано про это [ здесь ](http://www.sql-tutorial.ru/ru/book_rank_dense_rank_functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:20.909819Z",
     "start_time": "2023-02-05T17:48:20.481904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------------------+---------+-----+----------+------------------+----+\n",
      "| emp_id|  dept_id|salary|       salaries_list|nth_value|ntile|dense_rank|      percent_rank|rank|\n",
      "+-------+---------+------+--------------------+---------+-----+----------+------------------+----+\n",
      "|  Maria|  Finance|  3000|              [3000]|     NULL|    1|         1|               0.0|   1|\n",
      "|  Scott|  Finance|  3300|        [3000, 3300]|     3300|    1|         2|               0.5|   2|\n",
      "|    Jen|  Finance|  3900|  [3000, 3300, 3900]|     3300|    2|         3|               1.0|   3|\n",
      "|  Kumar|Marketing|  2000|              [2000]|     NULL|    1|         1|               0.0|   1|\n",
      "|   Jeff|Marketing|  3000|        [2000, 3000]|     3000|    2|         2|               1.0|   2|\n",
      "|  James|    Sales|  3000|              [3000]|     NULL|    1|         1|               0.0|   1|\n",
      "| Robert|    Sales|  4100|  [3000, 4100, 4100]|     4100|    1|         2|0.3333333333333333|   2|\n",
      "|   Saif|    Sales|  4100|  [3000, 4100, 4100]|     4100|    2|         2|0.3333333333333333|   2|\n",
      "|Michael|    Sales|  4600|[3000, 4100, 4100...|     4100|    2|         3|               1.0|   4|\n",
      "+-------+---------+------+--------------------+---------+-----+----------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df      \n",
    "        .withColumn('salaries_list', F.collect_list(df.salary).over(wspec))\n",
    "        .withColumn('nth_value', F.nth_value(df.salary, 2).over(wspec))\n",
    "        .withColumn('ntile', F.ntile(2).over(wspec))\n",
    "        .withColumn('dense_rank', F.dense_rank().over(wspec))\n",
    "        .withColumn('percent_rank', F.percent_rank().over(wspec))\n",
    "        .withColumn('rank', F.rank().over(wspec))\n",
    ").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `UDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущих примеров, при работе с элементами таблиц использовались встроенные методы обработки строк spark таблиц из модуля pyspark.sql.functions. Но фрейморк не запрещает определять свои кастомные функции обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:24.610784Z",
     "start_time": "2023-02-05T17:48:22.085861Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeniy/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|add_one(rating)|\n",
      "+---------------+\n",
      "|            6.0|\n",
      "|            4.0|\n",
      "|            4.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            4.0|\n",
      "|            6.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "|            4.0|\n",
      "|            5.0|\n",
      "|            6.0|\n",
      "|            5.0|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+--------+------+---------+--------+\n",
      "|user_id|movie_id|rating|timestamp|plus_one|\n",
      "+-------+--------+------+---------+--------+\n",
      "|      1|    1193|   5.0|978300760|     6.0|\n",
      "|      1|     661|   3.0|978302109|     4.0|\n",
      "|      1|     914|   3.0|978301968|     4.0|\n",
      "|      1|    3408|   4.0|978300275|     5.0|\n",
      "|      1|    2355|   5.0|978824291|     6.0|\n",
      "|      1|    1197|   3.0|978302268|     4.0|\n",
      "|      1|    1287|   5.0|978302039|     6.0|\n",
      "|      1|    2804|   5.0|978300719|     6.0|\n",
      "|      1|     594|   4.0|978302268|     5.0|\n",
      "|      1|     919|   4.0|978301368|     5.0|\n",
      "|      1|     595|   5.0|978824268|     6.0|\n",
      "|      1|     938|   4.0|978301752|     5.0|\n",
      "|      1|    2398|   4.0|978302281|     5.0|\n",
      "|      1|    2918|   4.0|978302124|     5.0|\n",
      "|      1|    1035|   5.0|978301753|     6.0|\n",
      "|      1|    2791|   4.0|978302188|     5.0|\n",
      "|      1|    2687|   3.0|978824268|     4.0|\n",
      "|      1|    2018|   4.0|978301777|     5.0|\n",
      "|      1|    3105|   5.0|978301713|     6.0|\n",
      "|      1|    2797|   4.0|978302039|     5.0|\n",
      "+-------+--------+------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeniy/.local/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@F.pandas_udf('double', F.PandasUDFType.SCALAR)\n",
    "def add_one(v):\n",
    "    return v + 1\n",
    "\n",
    "ratings_df.select(add_one(ratings_df.rating)).show()\n",
    "ratings_df.withColumn(\n",
    "    'plus_one', add_one(ratings_df.rating)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно определить свои агрегирующие функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:48:26.332484Z",
     "start_time": "2023-02-05T17:48:24.613454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 101:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+---------+\n",
      "|user_id|movie_id|      rating|timestamp|\n",
      "+-------+--------+------------+---------+\n",
      "|    181|      31|   0.8865249|977087101|\n",
      "|    195|      31|-0.113475084|991013952|\n",
      "|    203|      31|   -2.113475|976929358|\n",
      "|    223|      31|-0.113475084|976905652|\n",
      "|    268|      31|   0.8865249|976647137|\n",
      "|    368|      31|-0.113475084|976670975|\n",
      "|    517|      31|   0.8865249|976204301|\n",
      "|    524|      31|   -2.113475|976171096|\n",
      "|    528|      31|   1.8865249|980039160|\n",
      "|    531|      31|  -1.1134751|978973034|\n",
      "|    536|      31|-0.113475084|976137228|\n",
      "|    543|      31|   0.8865249|976159357|\n",
      "|    616|      31|-0.113475084|975802599|\n",
      "|    676|      31|   0.8865249|975684957|\n",
      "|    678|      31|   0.8865249|989241973|\n",
      "|    692|      31|-0.113475084|978375055|\n",
      "|    699|      31|-0.113475084|975563262|\n",
      "|    710|      31|-0.113475084|978586309|\n",
      "|    752|      31|   -2.113475|975461295|\n",
      "|    777|      31|-0.113475084|975520841|\n",
      "+-------+--------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@F.pandas_udf(ratings_df.schema, F.PandasUDFType.GROUPED_MAP)\n",
    "# Input/output are both a pandas.DataFrame\n",
    "def subtract_mean(pdf):\n",
    "    return pdf.assign(rating=pdf.rating - pdf.rating.mean())\n",
    "\n",
    "ratings_df.groupby('movie_id').apply(subtract_mean).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Формат Parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка больших данных увеличивает нагрузку на подсистему хранения — Hadoop хранит данные избыточно для достижения отказоустойчивости. Кроме дисков, нагружаются процессор, сеть, система ввода-вывода и так далее. По мере роста объема данных увеличивается и стоимость их обработки и хранения.\n",
    "\n",
    "Различные форматы файлов в Hadoop придуманы для решения именно этих проблем. Выбор подходящего формата файла может дать некоторые существенные преимущества:\n",
    "\n",
    " - Более быстрое время чтения.\n",
    " - Более быстрое время записи.\n",
    " - Разделяемые файлы.\n",
    " - Поддержка эволюции схем.\n",
    " - Расширенная поддержка сжатия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parquet](images/data_sizes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet - это столбчатый формат хранения данных. Это помогает повысить производительность,\n",
    "иногда значительно, разрешая хранение и доступ к данным для каждого столбца&\n",
    "\n",
    "Столбчатый формат более эффективен, когда вам нужно запросить из таблицы несколько столбцов. Он прочитает только необходимые столбцы, потому что они находятся по соседству. Таким образом, операции ввода-вывода сводятся к минимуму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parquet_columns](images/parquet_columns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним датафрейм ratings_df в формате Parquet и посмотрим его содержимое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"ratings_df_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После процедуры сохранения файла можно увидеть два файла в формате parquet. Количество сохранённых файлов равно количеству партиций RDD, которые были у данного датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выведем кол-во партиций\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprk позволяет при записи создавать разбиение данных на основе значений некоторого столбца. Таким образом каждое уникальное значение в нём создаст свою партцию.\n",
    "\n",
    "Зачастую это нужно для эффективной работы с нужной порцией данных, например, когда нужно обработать данные в одной партииции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "# сохраним данные и разобъём их на партции по полю salary\n",
    "df.write.partitionBy(\"salary\").mode(\"overwrite\").parquet(\"employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь если зайти в директорию employees, то можно увидедеть, что spark создал подпапки с названиями пратиций. Каждая папка хранит данные только своей партиции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
