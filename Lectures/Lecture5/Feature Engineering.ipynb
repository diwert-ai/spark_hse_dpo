{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Промышленное машинное обучение на Spark`\n",
    "## `Занятие 05: Feature Engineering`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О чём можно узнать из этого ноутбука:\n",
    "\n",
    "* Accumulator/Broadcast\n",
    "* Градиентный спуск\n",
    "* Винзоризация\n",
    "* Нормализация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:02.362387Z",
     "start_time": "2023-02-05T17:46:59.647723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/evgeniy/.local/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: pyarrow in /home/evgeniy/.local/lib/python3.10/site-packages (13.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/evgeniy/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/evgeniy/.local/lib/python3.10/site-packages (from pyarrow) (1.26.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pyspark pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-05T17:47:13.832913Z",
     "start_time": "2023-02-05T17:47:04.417093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 08:21:43 WARN Utils: Your hostname, vm-01 resolves to a loopback address: 127.0.1.1; using 10.128.0.16 instead (on interface eth0)\n",
      "23/11/11 08:21:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/11 08:21:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/11 08:21:58 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "23/11/11 08:21:58 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "23/11/11 08:21:58 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "23/11/11 08:21:58 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "23/11/11 08:21:58 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "        .set('spark.ui.port', '4050')\n",
    "        .setMaster('local[*]')\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Общие данные в Spark`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе в распределённых окружениях нужно учитывать, что разные воркеры имеют прямой доступ только к своим локальным данным. Как следствие, алгоритмы в таких системах должны учитывать отсутствие общей памяти в целом и проблемы синхронизации между отдельными процессами в частности.\n",
    "\n",
    "Для решения этой проблемы в Spark предложены два средства: Accumulator и Broadcast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Accumulator`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аккумуляторы в Spark представляют из себя общую переменную, которые отдельные воркеры могут обновлять, но не могут считывать (так как значение этой переменной однозначно не определено по причине необходимости дорогостоящей синхронизации между отдельными процессами). \n",
    "\n",
    "А таже поддерживают единственную операцию: `+=` (inplace add, `.__iadd__`) — коммутативную, ассоциативную операцию сложения. После того, как воркеры перестанут изменять переменную её значение доступно только на spark-драйвере через атрибут `.value`.\n",
    "\n",
    "Пример использования аккумуляторов — подсчёт общих статистик в процессе вычислений, например, суммарное значение функции потерь (см. пример с GD ниже), общее число слов в датасете и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создание такой переменной \n",
    "# возможно только через SparkContext\n",
    "acc = sc.accumulator(value=0)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Через атрибут `.value` посмотрим содержимое данной переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# отправляем на каждый из worker-нод данные\n",
    "rdd = sc.parallelize([1, 2, 3, -4, 5])\n",
    "# метод .add тоже самое что и +=\n",
    "rdd.foreach(lambda x: acc.add(x))\n",
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.value = 4\n",
    "acc.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущем примере для работы с accumulator использовалась анонимная lambda-функция. Но можно пользоваться и обычными именованными функциями с доступом к переменной через объявление её global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_sum = sc.accumulator(0)\n",
    "\n",
    "def count(x):\n",
    "    global acc_sum\n",
    "    acc_sum += x\n",
    "\n",
    "# теперь вместо lambda-функции указываем \n",
    "# имя созданной выше\n",
    "rdd.foreach(count)\n",
    "acc_sum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_cnt = sc.accumulator(0)\n",
    "rdd_02 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "rdd.foreach(lambda x: acc_cnt.add(1))\n",
    "rdd_02.foreach(lambda x: acc_cnt.add(1))\n",
    "acc_cnt.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В spark можно определять аккумуляторы собственного типа. Для этого необходимо создать класс-наследник класса AccumulatorParam. И реализовать в нём два метода:\n",
    "- **zero** - \"нулевое\" значение для созданного нового типа аккумулятора\n",
    "- **addInPlace** - описание логики сложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return [0.0] * len(value)\n",
    "    \n",
    "    def addInPlace(self, value_left, value_right):\n",
    "        for idx in range(len(value_left)):\n",
    "             value_left[idx] += value_right[idx]\n",
    "        return value_left\n",
    "\n",
    "# Первый аргумент - начальные значения переменной\n",
    "# Второй аргумент - Объект созданного пользовательского класса аккумулятора\n",
    "\n",
    "vector_acc = sc.accumulator([1.0, 2.0, 3.0], VectorAccumulatorParam())\n",
    "vector_acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0, 8.0, 9.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vector_add(x):\n",
    "    global vector_acc\n",
    "    vector_acc += [x] * 3\n",
    "    \n",
    "rdd = sc.parallelize([1, 2, 3])\n",
    "rdd.foreach(vector_add)\n",
    "vector_acc.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Концепция accumulator переменных не ограничивается только математической операцией сложения. Можно реализовывать свою произвольную логику аккумуляции данных.\n",
    "\n",
    "Например, как ниже реализована аккумуляция строк через механизм их конкатенации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StringAccumulator(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return value\n",
    "    def addInPlace(self, s1, s2):\n",
    "        s1 += s2\n",
    "        return s1\n",
    "\n",
    "string_acc = sc.accumulator(\"\", StringAccumulator())\n",
    "string_acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arkSp'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([\"S\", \"p\", \"a\", \"r\", \"k\"])\n",
    "rdd.foreach(lambda x: string_acc.add(x))\n",
    "string_acc.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как операция конкатенации строк не является коммутативной:  $x + y\\neq y + x$, то данный тип аккумулятора может выдавать разные результаты операций при повторных запусках, так как неизвестно какой из woker'ов первым произведёт сложение своей порции данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arkSp'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_acc.value = \"\"\n",
    "\n",
    "rdd = sc.parallelize([\"S\", \"p\", \"a\", \"r\", \"k\"])\n",
    "rdd.foreach(lambda x: string_acc.add(x))\n",
    "string_acc.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Broadcast`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![broadcast](images/broadcast.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнением к WO (Write Only) переменным (аккумуляторам) являются RO (Read Only) переменные.\n",
    "В Spark Read-only переменные называюся broadcast-переменными.\n",
    "Broadcast позволяет отправить на каждый воркер копию данных, которые затем можно использовать локально. Данная копия данных отправляется на каждый из worker-ов в момент её инициализации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NY': 'New York', 'CA': 'California', 'FL': 'Florida'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = {\"NY\": \"New York\", \"CA\": \"California\", \"FL\": \"Florida\"}\n",
    "broadcast_states = sc.broadcast(states)\n",
    "broadcast_states.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если во многих оперциях используются одни и те же значения заранее определённые значения данных, то имеет смысл данные значения поместить в broadcast-переменную и отправить на каждый из worker-нод. Таким образом уменьшив сетевые взаимодействия при обращении к этим данным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Типичным примером использования broadcast-переменных является применение их для отображения некоторых значений (lookup) как в примере  ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'Smith', 'USA', 'California'),\n",
       " ('Michael', 'Rose', 'USA', 'New York'),\n",
       " ('Robert', 'Williams', 'USA', 'California'),\n",
       " ('Maria', 'Jones', 'USA', 'Florida')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (\"James\", \"Smith\", \"USA\", \"CA\"),\n",
    "    (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
    "    (\"Robert\", \"Williams\", \"USA\", \"CA\"),\n",
    "    (\"Maria\", \"Jones\", \"USA\", \"FL\")\n",
    "]\n",
    "\n",
    "rdd_03 = sc.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcast_states.value[code]\n",
    "\n",
    "result = rdd_03.map(\n",
    "    lambda x: (x[0], x[1], x[2], state_convert(x[3]))\n",
    ").collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно использовать broadcast-переменные в задаче фильтрации данных, если количество данных, по которым нужно производить её невелико."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Michael', 'Rose', 'USA', 'NY'), ('Maria', 'Jones', 'USA', 'FL')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_states = sc.broadcast([\"NY\", \"FL\"])\n",
    "\n",
    "filtered = rdd_03.filter(\n",
    "    lambda x: x[3] in filtered_states.value\n",
    ").collect()\n",
    "filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Broadcast JOIN`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И ещё одним распространённым вариантом использования Broadcast является объединение таблиц, одна из которых \"маленького\" размера. В таком случае может оказаться выгоднее отправить копию меньшей таблицы на каждый воркер и выполнить Join локально, нежели чем выполнять распределённое объединение таблиц через обмен данными по сети.\n",
    "\n",
    "Нужно учитывать, что пересылка больших таблиц по сети может оказаться дорогостоящей, поэтому выбор между Broadcast Join и \"обычным\" Join зависит от конкретной конфигурации кластера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно задать размер DataFrame, при котором join будет автоматически происходить через broadcast этой таблицы\n",
    "# Размер задаётся в байтах. В данном случае — 100Мб.\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
    "\n",
    "# Значение -1 отключает Broadcast Join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем [m5-forecast датасет](https://www.kaggle.com/c/m5-forecasting-accuracy), как это было сделано в предыдущих лекциях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "folder_url = 'https://disk.yandex.lt/d/JnDy1h48pJI7IA'\n",
    "file_url = '/m5-forecating-accuracy.zip'\n",
    "# запрос ссылки на скачивание\n",
    "response = requests.get('https://cloud-api.yandex.net/v1/disk/public/resources/download',\n",
    "                 params={'public_key': folder_url, 'path': file_url}) \n",
    "# 'парсинг' ссылки на скачивание\n",
    "data_link = response.json()['href'] \t\n",
    "\n",
    "filename = 'm5-forecating-accuracy.zip'\n",
    "path = \"./m5-forecasting-accuracy\"\n",
    "\n",
    "# запускаем скачивание вызовом команды wget из python\n",
    "subprocess.run(\n",
    "    ['wget', '-O', filename, data_link], # команда для исполнения\n",
    "    stdout=subprocess.DEVNULL, # убираем печать отладочной информации\n",
    "    stderr=subprocess.STDOUT\n",
    ")\n",
    "\n",
    "#распакуем данные по пути path\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "# Зададим пути к файлам из датасета\n",
    "file_calendar = f\"{path}/calendar.csv\"\n",
    "file_validation = f\"{path}/sales_train_validation.csv\"\n",
    "file_evaluation = f\"{path}/sales_train_evaluation.csv\"\n",
    "file_prices = f\"{path}/sell_prices.csv\"\n",
    "file_calendar = f\"{path}/calendar.csv\"\n",
    "\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df_validation = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_validation)\n",
    ")\n",
    "df_evaluation = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_evaluation)\n",
    ")\n",
    "df_prices = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_prices)\n",
    ")\n",
    "df_calendar = (\n",
    "    spark.read.format(file_type)\n",
    "      .option(\"inferSchema\", infer_schema)\n",
    "      .option(\"header\", first_row_is_header)\n",
    "      .option(\"sep\", delimiter)\n",
    "      .load(file_calendar)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
       "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
       "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "2       0       2       3       0       1  \n",
       "3       1       3       0       2       6  \n",
       "4       0       0       2       1       0  \n",
       "\n",
       "[5 rows x 1947 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как ML-алгоритмы не работают с текстовыми данными, зачастую приходится преобразовывать текстовое предстваление данных в численное. Например, в датафрейме выше такими признаками являются колонки: cat_id, state_id и store_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:>                                                                                                                                                              (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   cat_id|\n",
      "+---------+\n",
      "|    FOODS|\n",
      "|HOUSEHOLD|\n",
      "|  HOBBIES|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "df_evaluation.select(\n",
    "    df_evaluation.cat_id\n",
    ").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим таблицу-отображение: текст -> число"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|   cat_id|code|\n",
      "+---------+----+\n",
      "|    FOODS|   0|\n",
      "|HOUSEHOLD|   2|\n",
      "|  HOBBIES|   3|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_id_hex =[\n",
    "    ('FOODS', 0),\n",
    "    ('HOUSEHOLD', 2),\n",
    "    ('HOBBIES', 3)\n",
    "]\n",
    "small_df = spark.createDataFrame(data=cat_id_hex, schema=['cat_id', 'code'])\n",
    "small_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы пометить, что некоторый Spark DataFrame должен быть отправлен на все worker-ноды, необходимо вопспользовтаться функцией `broadcast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1949 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1934  d_1935  d_1936  d_1937  d_1938  \\\n",
       "0       CA    0    0    0    0  ...       0       0       0       0       3   \n",
       "\n",
       "   d_1939  d_1940  d_1941   cat_id  code  \n",
       "0       3       0       1  HOBBIES     3  \n",
       "\n",
       "[1 rows x 1949 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_join_df = df_evaluation.join(\n",
    "  F.broadcast(small_df), small_df.cat_id == df_evaluation.cat_id\n",
    ")\n",
    "broadcast_join_df.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сравним количество операций между join с broadcast-dataframe и просто DataFrame, у которого данные разбиты по worker-нодам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [cat_id#39091], [cat_id#45007], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(cat_id#39091)\n",
      "   :  +- FileScan csv [id#39088,item_id#39089,dept_id#39090,cat_id#39091,store_id#39092,state_id#39093,d_1#39094,d_2#39095,d_3#39096,d_4#39097,d_5#39098,d_6#39099,d_7#39100,d_8#39101,d_9#39102,d_10#39103,d_11#39104,d_12#39105,d_13#39106,d_14#39107,d_15#39108,d_16#39109,d_17#39110,d_18#39111,... 1923 more fields] Batched: false, DataFilters: [isnotnull(cat_id#39091)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/evgeniy/github/spark_hse_dpo_2023/Lecture5/m5-forecasting-a..., PartitionFilters: [], PushedFilters: [IsNotNull(cat_id)], ReadSchema: struct<id:string,item_id:string,dept_id:string,cat_id:string,store_id:string,state_id:string,d_1:...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=685]\n",
      "      +- Filter isnotnull(cat_id#45007)\n",
      "         +- Scan ExistingRDD[cat_id#45007,code#45008L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broadcast_join_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [cat_id#39091], [cat_id#45007], Inner\n",
      "   :- Sort [cat_id#39091 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(cat_id#39091, 200), ENSURE_REQUIREMENTS, [plan_id=709]\n",
      "   :     +- Filter isnotnull(cat_id#39091)\n",
      "   :        +- FileScan csv [id#39088,item_id#39089,dept_id#39090,cat_id#39091,store_id#39092,state_id#39093,d_1#39094,d_2#39095,d_3#39096,d_4#39097,d_5#39098,d_6#39099,d_7#39100,d_8#39101,d_9#39102,d_10#39103,d_11#39104,d_12#39105,d_13#39106,d_14#39107,d_15#39108,d_16#39109,d_17#39110,d_18#39111,... 1923 more fields] Batched: false, DataFilters: [isnotnull(cat_id#39091)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/evgeniy/github/spark_hse_dpo_2023/Lecture5/m5-forecasting-a..., PartitionFilters: [], PushedFilters: [IsNotNull(cat_id)], ReadSchema: struct<id:string,item_id:string,dept_id:string,cat_id:string,store_id:string,state_id:string,d_1:...\n",
      "   +- Sort [cat_id#45007 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(cat_id#45007, 200), ENSURE_REQUIREMENTS, [plan_id=710]\n",
      "         +- Filter isnotnull(cat_id#45007)\n",
      "            +- Scan ExistingRDD[cat_id#45007,code#45008L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = df_evaluation.join(\n",
    "  small_df, small_df.cat_id == df_evaluation.cat_id\n",
    ")\n",
    "join_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно при join'е с broadcast перменной количество необходимых операций меньше"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Spark RDD Gradient Descent`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D = \\{(x_{i}, y_{i}) | x_{i} \\in \\mathbb{R}^{d}, y \\in \\mathbb{R}\\}_{1}^{n}\n",
    "$$\n",
    "$$\n",
    "{y}^{pred}_{i} = \\langle x, w \\rangle + b\n",
    "$$\n",
    "$$\n",
    "L_{i} = \\frac{1}{2} ({y}^{pred}_{i} - y_{i})^{2}\n",
    "$$\n",
    "$$\n",
    "\\mathfrak{L}(w, b) = \\frac{1}{n}\\sum\\limits_{i=1}^{n} L_{i} \\longrightarrow \\min_{w, b}\n",
    "$$\n",
    "\n",
    "Необходимо найти оптимальные $w \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из вариантов решения задачи: Градиентный Спуск (GD):\n",
    "\n",
    "$$\n",
    "\\space   w^{i+1} = w^{i} - \\alpha \\nabla_{w}\\mathfrak{L}\n",
    "$$\n",
    "$$\n",
    "\\space  b^{i+1} = b^{i} - \\alpha \\nabla_{b}\\mathfrak{L}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании линейной регрессии делается допущение о распределении данных:\n",
    "$$\n",
    "y_{i} \\sim \\mathcal{N}(\\langle x_{i}, w^{*} \\rangle, \\sigma^{2})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{w} L = (\\frac{\\partial L}{\\partial w_{1}}, ..., \\frac{\\partial L}{\\partial w_{d}})$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{w} \\mathfrak{L} = \\frac{1}{2n} \\sum\\limits_{i=1}^{n} \\nabla_{w}({y}^{pred}_{i} - y_{i})({y}^{pred}_{i} - y_{i}) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} ({y}^{pred}_{i} - y_{i}) \\nabla_{w}{y}^{pred}_{i} = \\{y_{i}^{pred} = \\langle x_{i}, w \\rangle + b \\} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} ({y}^{pred}_{i} - y_{i}) x_{i} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (\\langle x_{i}, w \\rangle + b - y_{i}) x_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем модельные данные, на которых провверим работу алгоритма градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((np.random.randn(1000, 10), np.ones((1000, 1))), axis=1)\n",
    "w_star = np.random.randn(X.shape[1])\n",
    "\n",
    "y = X.dot(w_star) + 0.001 * np.random.randn(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перепишем с математической формулировки на ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha=0.1, epochs=1):\n",
    "    # 1. Иницализируем начальные значения весов\n",
    "    # 2. Итеративно с кол-вом эпох N:\n",
    "    #     a. Вычисляем ошибку, считаем по ней градиент\n",
    "    #     b. Обновляем веса \n",
    "    # 3. На выходе из функции посчитанные веса w\n",
    "    \n",
    "    X_rdd = sc.parallelize(X).cache()\n",
    "    y_rdd = sc.parallelize(y).cache()\n",
    "    \n",
    "    n = X_rdd.count()\n",
    "    d = X_rdd.take(1)[0].shape[0]\n",
    "    \n",
    "    # Кэшируем результат вычислений, чтобы не перевычислять его на каждой итерации\n",
    "    X_y_rdd = X_rdd.zip(y_rdd).cache()\n",
    "    # инициализируем w нулевым вектором размерности d\n",
    "    w = np.zeros(d)\n",
    "    for epoch in range(epochs):\n",
    "        # на каждой эпохе инициализируем общее значение весов\n",
    "        # для всех worker-нод и размерность n\n",
    "        w_br = sc.broadcast(w)\n",
    "        n_br = sc.broadcast(n)\n",
    "        \n",
    "        total_error = sc.accumulator(0.0)\n",
    "        def grad_mapper(x_y, total_error):\n",
    "            delta = (np.sum(x_y[0] * w_br.value) - x_y[1])\n",
    "            error = (delta ** 2.0) / 2.0\n",
    "            total_error.add(error / n_br.value)\n",
    "            return x_y[0] * delta\n",
    "        \n",
    "        grad = X_y_rdd.map(lambda x: grad_mapper(x, total_error=total_error)).sum() / n\n",
    "        w = w - alpha * grad\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch: {0:d}, Loss {1:.3f}'.format(epoch, total_error.value / n))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient_descent(X, y, alpha=0.05, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07914287, -0.53380276,  2.03264089,  0.54738543,  1.30964064,\n",
       "       -0.69661485, -0.78671941,  1.17804389,  0.6113177 , -0.91477037,\n",
       "       -0.01177458])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_star"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Spark Winsorizing`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Винзоризация — это метод предобработки численных данных, при котором значения за пределами заданных квантилей заменяются на значения этих квантилей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![quantiles](q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, $X = \\{100, 6, 52, 26, 8, 81, 52, 15, 2, 74, 93, 82, 36, 22, 74, 90, 97, 50, 4, 40, 1\\}$.\n",
    "\n",
    "Винзоризация для $0.1$ и $0.9$ квантилей выполняется следующим образом:\n",
    "1. Определяем квантили: $q_{0.1} = 3, q_{0.9} = 93$\n",
    "2. Заменяем все значения меньше $3$ на $3$ и больше $93$ на $93$: \n",
    "\n",
    "$$\\hat{X} = \\{93, 6, 52, 26, 8, 81, 52, 15, 3, 74, 93, 82, 36, 22, 74, 90, 97, 50, 4, 40, 3\\}$$\n",
    "\n",
    "Такая предобработка убирает экстремальные значения и выбросы, что приводит к более надёжному посчёту статистик по выборке (матожидание, дисперисия и так далее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorizing(\n",
    "    df: pyspark.sql.dataframe.DataFrame, \n",
    "    column: str = 'sales',\n",
    "    lower_percentile: float = 0.1,\n",
    "    higher_percentile: float = 0.9\n",
    ") -> pyspark.sql.dataframe.DataFrame:\n",
    "    \n",
    "    wspec = Window().partitionBy()\n",
    "    \n",
    "    lp_column = '_'.join([column, 'lower_percentile'])\n",
    "    hp_column = '_'.join([column, 'higher_percentile'])\n",
    "    df = df.withColumns({\n",
    "        lp_column: F.percentile_approx(F.col(column), lower_percentile).over(wspec),\n",
    "        hp_column: F.percentile_approx(F.col(column), higher_percentile).over(wspec)\n",
    "    })\n",
    "    \n",
    "    df = (\n",
    "        df\n",
    "            .withColumn(\n",
    "                '_'.join([column, 'winsorized']),\n",
    "                \n",
    "                F.when(\n",
    "                    # если значение меньше левой квантили - заменяем\n",
    "                    F.col(column) < F.col(lp_column),\n",
    "                    F.col(lp_column)\n",
    "                ).otherwise(\n",
    "                    F.when(\n",
    "                        # если значение больше правой квантили - заменяем на неё\n",
    "                        F.col(column) > F.col(hp_column),\n",
    "                        F.col(hp_column)\n",
    "                    ).otherwise(\n",
    "                        # иначе \n",
    "                        F.col(column)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      sales|\n",
      "+-----------+\n",
      "| 0.77347701|\n",
      "| 0.77617723|\n",
      "|-0.26191574|\n",
      "| 0.06015559|\n",
      "|-0.18058041|\n",
      "| 1.15605904|\n",
      "|-0.54163328|\n",
      "| 0.83280377|\n",
      "|-0.69920523|\n",
      "|-0.33986035|\n",
      "|-0.94114708|\n",
      "|-0.88438698|\n",
      "| 1.18682329|\n",
      "| 1.21287342|\n",
      "|-0.82575258|\n",
      "|  0.5895868|\n",
      "|  -1.646899|\n",
      "| -1.5341987|\n",
      "|-0.94135006|\n",
      "|  0.5699716|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (0.77347701,  ),\n",
    "    (0.77617723, ),\n",
    "    (-0.26191574,  ),\n",
    "    (0.06015559, ),\n",
    "    (-0.18058041,),\n",
    "    (1.15605904, ),\n",
    "    (-0.54163328,  ),\n",
    "    (0.83280377,),\n",
    "    (-0.69920523, ),\n",
    "    (-0.33986035,),\n",
    "    (-0.94114708, ),\n",
    "    (-0.88438698,  ),\n",
    "    (1.18682329,  ),\n",
    "    (1.21287342, ),\n",
    "    (-0.82575258,),\n",
    "    (0.5895868, ),\n",
    "    (-1.646899, ),\n",
    "    (-1.5341987, ),\n",
    "    (-0.94135006,  ),\n",
    "    (0.5699716,)\n",
    "]\n",
    "df = spark.createDataFrame(data, ['sales'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/21 11:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/21 11:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/21 11:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/21 11:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/02/21 11:34:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+-----------+----------------------+-----------------------+----------------+\n",
      "|      sales|sales_lower_percentile|sales_higher_percentile|sales_winsorized|\n",
      "+-----------+----------------------+-----------------------+----------------+\n",
      "| 0.77347701|            -1.5341987|             1.15605904|      0.77347701|\n",
      "| 0.77617723|            -1.5341987|             1.15605904|      0.77617723|\n",
      "|-0.26191574|            -1.5341987|             1.15605904|     -0.26191574|\n",
      "| 0.06015559|            -1.5341987|             1.15605904|      0.06015559|\n",
      "|-0.18058041|            -1.5341987|             1.15605904|     -0.18058041|\n",
      "| 1.15605904|            -1.5341987|             1.15605904|      1.15605904|\n",
      "|-0.54163328|            -1.5341987|             1.15605904|     -0.54163328|\n",
      "| 0.83280377|            -1.5341987|             1.15605904|      0.83280377|\n",
      "|-0.69920523|            -1.5341987|             1.15605904|     -0.69920523|\n",
      "|-0.33986035|            -1.5341987|             1.15605904|     -0.33986035|\n",
      "|-0.94114708|            -1.5341987|             1.15605904|     -0.94114708|\n",
      "|-0.88438698|            -1.5341987|             1.15605904|     -0.88438698|\n",
      "| 1.18682329|            -1.5341987|             1.15605904|      1.15605904|\n",
      "| 1.21287342|            -1.5341987|             1.15605904|      1.15605904|\n",
      "|-0.82575258|            -1.5341987|             1.15605904|     -0.82575258|\n",
      "|  0.5895868|            -1.5341987|             1.15605904|       0.5895868|\n",
      "|  -1.646899|            -1.5341987|             1.15605904|      -1.5341987|\n",
      "| -1.5341987|            -1.5341987|             1.15605904|      -1.5341987|\n",
      "|-0.94135006|            -1.5341987|             1.15605904|     -0.94135006|\n",
      "|  0.5699716|            -1.5341987|             1.15605904|       0.5699716|\n",
      "+-----------+----------------------+-----------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "winsorizing(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Нормализация данных`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Standard Scaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отнормализуем данные, так чтобы их среднее было нулевым, а стандартное отклонение единичным.\n",
    "\n",
    "$$ X = \\frac{X-\\mu}{\\sigma}$$\n",
    "\n",
    "- $\\mu = EX$ - среднее значение признака, посчитанного по всем данным\n",
    "\n",
    "- $\\sigma = \\sqrt{DX} = \\sqrt{E[X - EX]^{2}}$ - его стадартное нормальное отклонение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![normalization](images/normalization.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|             sales|\n",
      "+------------------+\n",
      "| 4.142494366086895|\n",
      "| 9.639546895246765|\n",
      "| 18.25011321949261|\n",
      "|15.352734159760217|\n",
      "|19.405881111477253|\n",
      "| 10.28101061161016|\n",
      "|13.575668268235043|\n",
      "|16.256219883800973|\n",
      "|14.069543501302437|\n",
      "|11.595794759712811|\n",
      "|11.025936207941449|\n",
      "|3.0266474902194096|\n",
      "| 8.129449547044569|\n",
      "| 7.974603647706955|\n",
      "|3.9665375554392357|\n",
      "| 9.840458658422351|\n",
      "|1.8000164983523548|\n",
      "|5.7111954986306195|\n",
      "|1.1489917593161625|\n",
      "|19.517643212735482|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# сгенерируем модельные данные из равномерного распредления на отрезку [0, 20]\n",
    "\n",
    "data = [\n",
    "    (random.uniform(0, 20), )\n",
    "    for i in range(20)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, ['sales'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, input_column, output_column):\n",
    "    \n",
    "    wspec = Window().partitionBy()\n",
    "    \n",
    "    df = df.withColumn(\"mu\", F.mean(input_column).over(wspec))\n",
    "    df = df.withColumn(\"sigma\", F.sqrt(F.mean((F.col(input_column) - F.col(\"mu\")) ** 2).over(wspec)))\n",
    "\n",
    "    df = df.withColumn(output_column, (F.col(input_column) - F.col(\"mu\")) / F.col(\"sigma\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 08:11:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:11:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:11:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:11:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:11:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------+--------------------+\n",
      "|             sales|                mu|            sigma|    normalized_sales|\n",
      "+------------------+------------------+-----------------+--------------------+\n",
      "| 4.142494366086895|10.235524342626688|5.647561211979469| -1.0788780763660262|\n",
      "| 9.639546895246765|10.235524342626688|5.647561211979469|-0.10552828468963736|\n",
      "| 18.25011321949261|10.235524342626688|5.647561211979469|  1.4191238617946405|\n",
      "|15.352734159760217|10.235524342626688|5.647561211979469|  0.9060919616557725|\n",
      "|19.405881111477253|10.235524342626688|5.647561211979469|  1.6237728861439569|\n",
      "| 10.28101061161016|10.235524342626688|5.647561211979469|0.008054143598654218|\n",
      "|13.575668268235043|10.235524342626688|5.647561211979469|  0.5914312037067122|\n",
      "|16.256219883800973|10.235524342626688|5.647561211979469|  1.0660699929030133|\n",
      "|14.069543501302437|10.235524342626688|5.647561211979469|  0.6788804963358558|\n",
      "|11.595794759712811|10.235524342626688|5.647561211979469| 0.24085979169216462|\n",
      "|11.025936207941449|10.235524342626688|5.647561211979469| 0.13995631665543679|\n",
      "|3.0266474902194096|10.235524342626688|5.647561211979469|   -1.27645838297706|\n",
      "| 8.129449547044569|10.235524342626688|5.647561211979469|-0.37291756858071134|\n",
      "| 7.974603647706955|10.235524342626688|5.647561211979469| -0.4003357573396324|\n",
      "|3.9665375554392357|10.235524342626688|5.647561211979469| -1.1100343231145207|\n",
      "| 9.840458658422351|10.235524342626688|5.647561211979469|-0.06995332487345739|\n",
      "|1.8000164983523548|10.235524342626688|5.647561211979469| -1.4936549649751718|\n",
      "|5.7111954986306195|10.235524342626688|5.647561211979469| -0.8011119621685857|\n",
      "|1.1489917593161625|10.235524342626688|5.647561211979469| -1.6089303404160356|\n",
      "|19.517643212735482|10.235524342626688|5.647561211979469|  1.6435623310146317|\n",
      "+------------------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 08:11:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:11:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:11:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:11:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "normalized_df = normalize(df, \"sales\", \"normalized_sales\")\n",
    "normalized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|sales             |mu                     |sigma             |normalized_sales    |tmp                 |\n",
      "+------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|4.142494366086895 |-1.1102230246251566E-17|0.9999999999999999|-1.0788780763660262 |-1.0788780763660264 |\n",
      "|9.639546895246765 |-1.1102230246251566E-17|0.9999999999999999|-0.10552828468963736|-0.10552828468963736|\n",
      "|18.25011321949261 |-1.1102230246251566E-17|0.9999999999999999|1.4191238617946405  |1.4191238617946407  |\n",
      "|15.352734159760217|-1.1102230246251566E-17|0.9999999999999999|0.9060919616557725  |0.9060919616557727  |\n",
      "|19.405881111477253|-1.1102230246251566E-17|0.9999999999999999|1.6237728861439569  |1.623772886143957   |\n",
      "|10.28101061161016 |-1.1102230246251566E-17|0.9999999999999999|0.008054143598654218|0.00805414359865423 |\n",
      "|13.575668268235043|-1.1102230246251566E-17|0.9999999999999999|0.5914312037067122  |0.5914312037067123  |\n",
      "|16.256219883800973|-1.1102230246251566E-17|0.9999999999999999|1.0660699929030133  |1.0660699929030135  |\n",
      "|14.069543501302437|-1.1102230246251566E-17|0.9999999999999999|0.6788804963358558  |0.6788804963358559  |\n",
      "|11.595794759712811|-1.1102230246251566E-17|0.9999999999999999|0.24085979169216462 |0.24085979169216465 |\n",
      "|11.025936207941449|-1.1102230246251566E-17|0.9999999999999999|0.13995631665543679 |0.1399563166554368  |\n",
      "|3.0266474902194096|-1.1102230246251566E-17|0.9999999999999999|-1.27645838297706   |-1.2764583829770602 |\n",
      "|8.129449547044569 |-1.1102230246251566E-17|0.9999999999999999|-0.37291756858071134|-0.3729175685807114 |\n",
      "|7.974603647706955 |-1.1102230246251566E-17|0.9999999999999999|-0.4003357573396324 |-0.40033575733963245|\n",
      "|3.9665375554392357|-1.1102230246251566E-17|0.9999999999999999|-1.1100343231145207 |-1.1100343231145209 |\n",
      "|9.840458658422351 |-1.1102230246251566E-17|0.9999999999999999|-0.06995332487345739|-0.06995332487345739|\n",
      "|1.8000164983523548|-1.1102230246251566E-17|0.9999999999999999|-1.4936549649751718 |-1.493654964975172  |\n",
      "|5.7111954986306195|-1.1102230246251566E-17|0.9999999999999999|-0.8011119621685857 |-0.8011119621685858 |\n",
      "|1.1489917593161625|-1.1102230246251566E-17|0.9999999999999999|-1.6089303404160356 |-1.6089303404160358 |\n",
      "|19.517643212735482|-1.1102230246251566E-17|0.9999999999999999|1.6435623310146317  |1.643562331014632   |\n",
      "+------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:12:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# посмотрим чему теперь равны среднее и стандартное отклонение для нормализоавнного датафрейма\n",
    "normalize(normalized_df, \"normalized_sales\", \"tmp\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `MinMax Scaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо преобразовать данные к заранее заданному интервалу, например, к единичному интервалу [0, 1], то тут подойдёт min-max нормализация.\n",
    "\n",
    "$$X = \\frac{X - min(X)}{max(X) - min(X)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(df, input_column, output_column):\n",
    "    \n",
    "    wspec = Window().partitionBy()\n",
    "    \n",
    "    df = df.withColumn(\"max\", F.max(input_column).over(wspec))\n",
    "    df = df.withColumn(\"min\", F.min(input_column).over(wspec))\n",
    "\n",
    "    df = df.withColumn(output_column, (F.col(input_column) - F.col(\"min\")) / (F.col(\"max\") - F.col(\"min\")))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 08:17:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:17:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:17:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+-------------------+\n",
      "|             sales|               max|               min|   normalized_sales|\n",
      "+------------------+------------------+------------------+-------------------+\n",
      "| 4.142494366086895|19.517643212735482|1.1489917593161625| 0.1629680117978118|\n",
      "| 9.639546895246765|19.517643212735482|1.1489917593161625| 0.4622307281218561|\n",
      "| 18.25011321949261|19.517643212735482|1.1489917593161625| 0.9309949346876568|\n",
      "|15.352734159760217|19.517643212735482|1.1489917593161625| 0.7732599443384851|\n",
      "|19.405881111477253|19.517643212735482|1.1489917593161625| 0.9939156066224216|\n",
      "| 10.28101061161016|19.517643212735482|1.1489917593161625| 0.4971523835297161|\n",
      "|13.575668268235043|19.517643212735482|1.1489917593161625| 0.6765154502730607|\n",
      "|16.256219883800973|19.517643212735482|1.1489917593161625|  0.822446229261572|\n",
      "|14.069543501302437|19.517643212735482|1.1489917593161625| 0.7034023033618573|\n",
      "|11.595794759712811|19.517643212735482|1.1489917593161625| 0.5687299923398558|\n",
      "|11.025936207941449|19.517643212735482|1.1489917593161625| 0.5377065634715769|\n",
      "|3.0266474902194096|19.517643212735482|1.1489917593161625|0.10222066305002059|\n",
      "| 8.129449547044569|19.517643212735482|1.1489917593161625|0.38002015583070997|\n",
      "| 7.974603647706955|19.517643212735482|1.1489917593161625|0.37159025558842573|\n",
      "|3.9665375554392357|19.517643212735482|1.1489917593161625|0.15338882134423582|\n",
      "| 9.840458658422351|19.517643212735482|1.1489917593161625| 0.4731684806120198|\n",
      "|1.8000164983523548|19.517643212735482|1.1489917593161625|0.03544216300729057|\n",
      "|5.7111954986306195|19.517643212735482|1.1489917593161625|0.24836900797446426|\n",
      "|1.1489917593161625|19.517643212735482|1.1489917593161625|                0.0|\n",
      "|19.517643212735482|19.517643212735482|1.1489917593161625|                1.0|\n",
      "+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 08:17:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/11/11 08:17:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "normalized_df = min_max_normalize(df, \"sales\", \"normalized_sales\")\n",
    "normalized_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
